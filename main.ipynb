{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30d87e5c143b471faed6cad1481cce8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_638acc671d5a4f17a7b358af73dd9c46",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5f64181564494ee492374a52c4a209e0",
              "IPY_MODEL_0393cfde8905430596922a6b7134885d"
            ]
          }
        },
        "638acc671d5a4f17a7b358af73dd9c46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f64181564494ee492374a52c4a209e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_98873c8f22fe463d9bda3cf979d8b8a3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95ec8ef9d8004dcd9ae78b9bb5214f2a"
          }
        },
        "0393cfde8905430596922a6b7134885d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5a62ef1f2c940d785ccf127cb65bc25",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.77MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04674a47dd7b4ff69ca202a35aa75c76"
          }
        },
        "98873c8f22fe463d9bda3cf979d8b8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95ec8ef9d8004dcd9ae78b9bb5214f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5a62ef1f2c940d785ccf127cb65bc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04674a47dd7b4ff69ca202a35aa75c76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b80e3f625304638962dd714364ab95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e188ff22a564ed894a053ef7d2b81db",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4862cbe82b7f462fa28019e18888e99c",
              "IPY_MODEL_2963ea50895d4ec096d9fa3a8af5a6f3"
            ]
          }
        },
        "4e188ff22a564ed894a053ef7d2b81db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4862cbe82b7f462fa28019e18888e99c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d65c6b99ff64a2ba38ae42c534a9064",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_488fced3f5754de08c9c1327170c6a35"
          }
        },
        "2963ea50895d4ec096d9fa3a8af5a6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab61266a4ea24569a57f3717927b5f0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:00&lt;00:00, 8.23kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc2ed7f4b33d47f68430950a3638fcbb"
          }
        },
        "9d65c6b99ff64a2ba38ae42c534a9064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "488fced3f5754de08c9c1327170c6a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab61266a4ea24569a57f3717927b5f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc2ed7f4b33d47f68430950a3638fcbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07176c25d9b8461f91ea8e3dbb23b9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_72677f2f3550439a80c31911ab59531c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_faf2b48f606946bc948d7bceb778f079",
              "IPY_MODEL_d7487957e8274f15b955c0d0507574f6"
            ]
          }
        },
        "72677f2f3550439a80c31911ab59531c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "faf2b48f606946bc948d7bceb778f079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_961a6ffc22d04c07b0ddc3e4b8f3d691",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb1900d1787e4a258cc29fcbf955131a"
          }
        },
        "d7487957e8274f15b955c0d0507574f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_efcedf053bc4452985033d95a356abea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:04&lt;00:00, 56.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_257f83a2c2914f65a5fb4bf28ffac4d5"
          }
        },
        "961a6ffc22d04c07b0ddc3e4b8f3d691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb1900d1787e4a258cc29fcbf955131a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efcedf053bc4452985033d95a356abea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "257f83a2c2914f65a5fb4bf28ffac4d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willburr/humorous-headlines/blob/box-plot-gen/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEewKMPzm6ys"
      },
      "source": [
        "### Coursework coding instructions (please also see full coursework spec)\n",
        "\n",
        "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
        "\n",
        "For the task you choose you will need to do two approaches:\n",
        "  - Approach 1, which can use use pre-trained embeddings / models\n",
        "  - Approach 2, which should not use any pre-trained embeddings or models\n",
        "We should be able to run both approaches from the same colab file\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "\n",
        "#### Reproducibility:\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "Good luck! We are really looking forward to seeing your reports and your model code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd1-94s1MMgw"
      },
      "source": [
        "## Download and install Transformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTEI9ZFUwt0g",
        "outputId": "3109b864-6293-4989-ca7a-85d5d29d1c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\r\u001b[K     |▏                               | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 24.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 28.2MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 26.1MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 21.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71kB 20.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 21.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92kB 19.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102kB 19.1MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 19.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122kB 19.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133kB 19.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 143kB 19.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153kB 19.1MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 19.1MB/s eta 0:00:01\r\u001b[K     |███                             | 174kB 19.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 184kB 19.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 194kB 19.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 204kB 19.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 215kB 19.1MB/s eta 0:00:01\r\u001b[K     |████                            | 225kB 19.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 235kB 19.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 245kB 19.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 256kB 19.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 266kB 19.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 276kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 286kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 296kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 307kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 317kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 327kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 337kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 348kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 358kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 368kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 378kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 389kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 399kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 409kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 419kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 430kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 440kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 450kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 460kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 471kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 481kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 491kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 501kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 512kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 522kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 532kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 542kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 552kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 563kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 573kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 583kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 593kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 604kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 614kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 624kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 634kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 645kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 655kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 665kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 675kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 686kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 696kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 706kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 716kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 727kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 737kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 747kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 757kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 768kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 778kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 788kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 798kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 808kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 819kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 829kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 839kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 849kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 860kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 870kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 880kB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 890kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 901kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 911kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 921kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 931kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 942kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 952kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 962kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 972kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 983kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 993kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.7MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.8MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.8MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.8MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.8MB 19.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.8MB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=5ddf1505a7e484dfb95fa55cf28818e717a3ea4a30bc64815eb0317aad83be9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSraR3PdJ2i9"
      },
      "source": [
        "# Project  Set-up\r\n",
        "\r\n",
        "The code blocks below provide the imports and set-up steps to run later sections. We import necessary libraries, set pytorch to use the GPU and load and unzip the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFcxAGnSm6y2",
        "outputId": "257cf830-6e81-4d64-e290-8d74f3d771e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import torch.optim as optim\n",
        "import codecs\n",
        "import tqdm\n",
        "\n",
        "# Helper code to perform stopword removal\n",
        "!pip install nltk \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Imports for use of BERT\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM2SBsBzm6y3"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVWAKu6ALaCL"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "Here we download the data files, load them into pandas and examine the first few columns of the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AEMK9jQjem-",
        "outputId": "5514ef0e-3479-4060-fb17-0caf89ffcd1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget -O train.csv https://drive.google.com/u/0/uc?id=1KS6Cxl4CJnSLkMcdgbnmEdsStlVzacWX&export=download\n",
        "!wget -O dev.csv https://drive.google.com/u/0/uc?id=19WKir5IRn83NMcVgvNDrmgcDj1UIv7kt&export=download\n",
        "!wget -O test.csv https://drive.google.com/u/0/uc?id=11pyqg27tGRC1iDo26C2b01bMETqdUwuf&export=download"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-23 18:44:46--  https://drive.google.com/u/0/uc?id=1KS6Cxl4CJnSLkMcdgbnmEdsStlVzacWX\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.5.238, 2607:f8b0:4004:804::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.5.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lpjckf9kp2arvg3u7ribpjtandj6t8ue/1614105825000/13802342090854404605/*/1KS6Cxl4CJnSLkMcdgbnmEdsStlVzacWX [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-23 18:44:47--  https://doc-08-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lpjckf9kp2arvg3u7ribpjtandj6t8ue/1614105825000/13802342090854404605/*/1KS6Cxl4CJnSLkMcdgbnmEdsStlVzacWX\n",
            "Resolving doc-08-cc-docs.googleusercontent.com (doc-08-cc-docs.googleusercontent.com)... 172.217.13.225, 2607:f8b0:4004:809::2001\n",
            "Connecting to doc-08-cc-docs.googleusercontent.com (doc-08-cc-docs.googleusercontent.com)|172.217.13.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1859231 (1.8M) [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   1.77M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-02-23 18:44:48 (32.8 MB/s) - ‘train.csv’ saved [1859231/1859231]\n",
            "\n",
            "--2021-02-23 18:44:48--  https://drive.google.com/u/0/uc?id=19WKir5IRn83NMcVgvNDrmgcDj1UIv7kt\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.5.238, 2607:f8b0:4004:804::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.5.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8su9uul6p89k638mouhqe34buttur7m5/1614105825000/13802342090854404605/*/19WKir5IRn83NMcVgvNDrmgcDj1UIv7kt [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-23 18:44:48--  https://doc-0c-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8su9uul6p89k638mouhqe34buttur7m5/1614105825000/13802342090854404605/*/19WKir5IRn83NMcVgvNDrmgcDj1UIv7kt\n",
            "Resolving doc-0c-cc-docs.googleusercontent.com (doc-0c-cc-docs.googleusercontent.com)... 172.217.13.225, 2607:f8b0:4004:809::2001\n",
            "Connecting to doc-0c-cc-docs.googleusercontent.com (doc-0c-cc-docs.googleusercontent.com)|172.217.13.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 424235 (414K) [text/csv]\n",
            "Saving to: ‘dev.csv’\n",
            "\n",
            "dev.csv             100%[===================>] 414.29K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-02-23 18:44:48 (11.0 MB/s) - ‘dev.csv’ saved [424235/424235]\n",
            "\n",
            "--2021-02-23 18:44:48--  https://drive.google.com/u/0/uc?id=11pyqg27tGRC1iDo26C2b01bMETqdUwuf\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.5.238, 2607:f8b0:4004:804::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.5.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-70-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6i6t1v655cn8sc7rfbm0248g9a225gih/1614105825000/18042724966187936417/*/11pyqg27tGRC1iDo26C2b01bMETqdUwuf [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-23 18:44:49--  https://doc-04-70-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6i6t1v655cn8sc7rfbm0248g9a225gih/1614105825000/18042724966187936417/*/11pyqg27tGRC1iDo26C2b01bMETqdUwuf\n",
            "Resolving doc-04-70-docs.googleusercontent.com (doc-04-70-docs.googleusercontent.com)... 172.217.13.225, 2607:f8b0:4004:809::2001\n",
            "Connecting to doc-04-70-docs.googleusercontent.com (doc-04-70-docs.googleusercontent.com)|172.217.13.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 525642 (513K) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 513.32K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-02-23 18:44:50 (12.7 MB/s) - ‘test.csv’ saved [525642/525642]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VU17WM0m6y3"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TevikPkl010",
        "outputId": "3dbaa7df-faf5-4083-e96e-5db0e3c31017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Take a look at the data\r\n",
        "test_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original1</th>\n",
              "      <th>edit1</th>\n",
              "      <th>original2</th>\n",
              "      <th>edit2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>704-2704</td>\n",
              "      <td>\" Pence Is Trying to Control Republican Politi...</td>\n",
              "      <td>barbers</td>\n",
              "      <td>\" Pence Is Trying to &lt;Control/&gt; Republican Pol...</td>\n",
              "      <td>Bungle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>704-14395</td>\n",
              "      <td>\" Pence Is Trying to Control Republican Politi...</td>\n",
              "      <td>barbers</td>\n",
              "      <td>\" &lt;Pence/&gt; Is Trying to Control Republican Pol...</td>\n",
              "      <td>Witch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2704-14395</td>\n",
              "      <td>\" Pence Is Trying to &lt;Control/&gt; Republican Pol...</td>\n",
              "      <td>Bungle</td>\n",
              "      <td>\" &lt;Pence/&gt; Is Trying to Control Republican Pol...</td>\n",
              "      <td>Witch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11098-10186</td>\n",
              "      <td>\" There is no Man Behind the Curtain , \" says ...</td>\n",
              "      <td>elephant</td>\n",
              "      <td>\" There is no Man Behind the Curtain , \" says ...</td>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11098-2118</td>\n",
              "      <td>\" There is no Man Behind the Curtain , \" says ...</td>\n",
              "      <td>elephant</td>\n",
              "      <td>\" There is no Man Behind the Curtain , \" says ...</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ...   edit2\n",
              "0     704-2704  ...  Bungle\n",
              "1    704-14395  ...   Witch\n",
              "2   2704-14395  ...   Witch\n",
              "3  11098-10186  ...   woman\n",
              "4   11098-2118  ...     man\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSRS6PLHLAtf"
      },
      "source": [
        "## Download and unzip GLOVE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydxDrWAEpSjo"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlEA5Hhym6y1",
        "outputId": "103800c5-934c-49f7-99e9-8c878ff6965d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-23 18:44:50--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-23 18:44:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-23 18:44:50--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.21MB/s    in 6m 51s  \n",
            "\n",
            "2021-02-23 18:51:42 (2.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riXUwYsk9-TO",
        "outputId": "8e338366-dad8-41fa-8858-7e9a1573c395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip '/content/drive/MyDrive/cw/glove.6B.zip'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/cw/glove.6B.zip, /content/drive/MyDrive/cw/glove.6B.zip.zip or /content/drive/MyDrive/cw/glove.6B.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ErHEDpRMfSv"
      },
      "source": [
        "## Training Evaluation Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZKyXkuFm6y5"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch, optimizer, scheduler=None):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Training model.\")\n",
        "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "        \n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_correct = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "            feature, target = batch\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            correct, __ = model_performance(np.argmax(predictions.detach().cpu().numpy(), axis=1), target.detach().cpu().numpy())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if scheduler:\n",
        "              scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_correct += correct\n",
        "\n",
        "        valid_loss, valid_acc, __, __ = eval(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_acc = epoch_loss / no_observations, epoch_correct / no_observations\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train Accuracy: {epoch_acc:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. Accuracy: {valid_acc:.2f} |')\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "        val_losses.append(valid_loss)\n",
        "        val_accs.append(valid_acc)\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUmf4mSTm6y6"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "            \n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            correct, __ = model_performance(np.argmax(pred, axis=1), trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_correct += correct\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_correct/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wre27ok7m6y7"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    correct_answers = (output == target)\n",
        "    correct = sum(correct_answers)\n",
        "    acc = np.true_divide(correct,len(output))\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| Acc: {acc:.2f} ')\n",
        "\n",
        "    return correct, acc"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWsWCL8NNBR0"
      },
      "source": [
        "def eval_test(data_iter, model, ids, filename):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    pred_all = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            \n",
        "            batch = batch.to(device)\n",
        "            # for RNN:\n",
        "            model.batch_size = batch.shape[1]\n",
        "\n",
        "            predictions = model(batch).squeeze(1)\n",
        "\n",
        "            pred = predictions.detach().cpu().numpy()\n",
        "\n",
        "            pred_all.extend(pred)\n",
        "\n",
        "    pred_all = np.argmax(np.array(pred_all), axis=1)\n",
        "\n",
        "    combined = list(zip(ids, pred_all))\n",
        "\n",
        "    res = pd.DataFrame(combined, columns=['id', 'pred'])\n",
        "    res.set_index('id', inplace=True, drop=True)\n",
        "\n",
        "    res.to_csv(filename + '.csv')\n",
        "\n",
        "    return res"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L9ZReWeMo7M"
      },
      "source": [
        "# Data augmentation and Vocab Preparation\n",
        "\n",
        "The following section provides the functions used to augment the data such that we flip the ordering of input sentences and flip the resulting label. We also define the classes and functions used for creating the vocabulary data structure that maintains mappings between words and indexes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2qnkCm2M5QK"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "The substitute function performs the injection of the edit word into the original sentence. This is then used in get_edited_df to return the dataframe with columns containing the edited sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkq5h2NF-VnS"
      },
      "source": [
        "import re\n",
        "\n",
        "def substitute(sentence, edit):\n",
        "  open_pos = sentence.find('<')\n",
        "  close_pos = sentence.find('>')\n",
        "  sub = sentence.replace(sentence[open_pos: close_pos + 1], edit)\n",
        "  return sub\n",
        "\n",
        "def get_edited_df(df, test=False):\n",
        "  id = df['id']\n",
        "  edited1 = df.apply(lambda x:substitute(x['original1'], x['edit1']), axis=1)\n",
        "  edited2 = df.apply(lambda x:substitute(x['original2'], x['edit2']), axis=1)\n",
        "  combined = list(zip(edited1,edited2))\n",
        "  if test:\n",
        "    id = df['id']\n",
        "    combined = list(zip(id,edited1,edited2))\n",
        "    return pd.DataFrame(combined, columns=['id','edited1', 'edited2'])\n",
        "  return pd.DataFrame(combined, columns=['edited1', 'edited2'])\n",
        "\n",
        "edited_train_df = get_edited_df(train_df)\n",
        "edited_test_df = get_edited_df(test_df, True)\n",
        "\n",
        "edited_train_df.head()\n",
        "\n",
        "# We set our training data and test data\n",
        "training_data = edited_train_df.values.tolist()\n",
        "labels = list(train_df['label'])\n",
        "test_data = np.array(edited_test_df.values.tolist())[:,1:].tolist()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp28SsNZ5HQR"
      },
      "source": [
        "## Define Vocab Class\r\n",
        "\r\n",
        "This class stores mappings from word to index and index to words. It also provides utility functions to build the mappings from files or a list data structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3q9gLjl5TOq"
      },
      "source": [
        "class Vocabulary(object):\r\n",
        "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\r\n",
        "  def __init__(self):\r\n",
        "    # Mapping from tokens to integers\r\n",
        "    self._word2idx = {}\r\n",
        "\r\n",
        "    # Reverse-mapping from integers to tokens\r\n",
        "    self.idx2word = []\r\n",
        "\r\n",
        "  def word2idx(self, word, default=None):\r\n",
        "    \"\"\"Returns the integer ID of the word or default if not found.\"\"\"\r\n",
        "    return self._word2idx.get(word, default)\r\n",
        "\r\n",
        "  def add_word(self, word):\r\n",
        "    \"\"\"Adds the `word` into the vocabulary.\"\"\"\r\n",
        "    if word not in self._word2idx:\r\n",
        "      self.idx2word.append(word)\r\n",
        "      self._word2idx[word] = len(self.idx2word) - 1\r\n",
        "\r\n",
        "  def build_from_list(self, words):\r\n",
        "    for word in words:\r\n",
        "      self.add_word(word)\r\n",
        "\r\n",
        "  def build_from_file(self, fname):\r\n",
        "    \"\"\"Builds a vocabulary from a given corpus file.\"\"\"\r\n",
        "    with open(fname) as f:\r\n",
        "      for line in f:\r\n",
        "        words = line.strip().split()\r\n",
        "        for word in words:\r\n",
        "          self.add_word(word)\r\n",
        "\r\n",
        "  def convert_idxs_to_words(self, idxs):\r\n",
        "    \"\"\"Converts a list of indices to words.\"\"\"\r\n",
        "    return ' '.join(self.idx2word[idx] for idx in idxs)\r\n",
        "\r\n",
        "  def convert_words_to_idxs(self, words):\r\n",
        "    \"\"\"Converts a list of words to a list of indices.\"\"\"\r\n",
        "    return [self.word2idx(w) for w in words]\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    \"\"\"Returns the size of the vocabulary.\"\"\"\r\n",
        "    return len(self.idx2word)\r\n",
        "  \r\n",
        "  def __repr__(self):\r\n",
        "    return \"Vocabulary with {} items\".format(self.__len__())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbw3d6ZFM8_E"
      },
      "source": [
        "## Vocab Preparation\n",
        "\n",
        "create_vocab allows for the tokenization of the corpus and creation of a list of words to be passed to the vocabulary. It provides two flags which the user can set to execute data cleaning operations of punctuation and stopword removal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zub4c6BJm6y8"
      },
      "source": [
        "def create_vocab(data, remove_punc=True, remove_stopwords=True):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    for sentence_pair in data:\n",
        "        tokenized_sentence_pair = []\n",
        "        for sentence in sentence_pair:\n",
        "            tokenized_sentence = []\n",
        "\n",
        "            # Split on spaces\n",
        "            tokens = sentence.split(' ')\n",
        "            \n",
        "            # Remove punctuation\n",
        "            if remove_punc:\n",
        "              tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "            # Remove stopwords\n",
        "            if remove_stopwords:\n",
        "              tokens = [word for word in tokens if not word in stop_words]\n",
        "\n",
        "            for token in tokens: \n",
        "\n",
        "                tokenized_sentence.append(token.lower())\n",
        "\n",
        "            tokenized_sentence_pair.append(tokenized_sentence)\n",
        "        tokenized_corpus.append(tokenized_sentence_pair)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence_pair in tokenized_corpus:\n",
        "\n",
        "        for sentence in sentence_pair:\n",
        "\n",
        "            for token in sentence:\n",
        "\n",
        "                if token.lower() not in vocabulary:\n",
        "\n",
        "                    vocabulary.append(token.lower())\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTQ_arcRNEjK"
      },
      "source": [
        "## Load the Dataset\n",
        "\n",
        "In this section, we present the code that is used to create train and validation pytorch dataloaders with the data augmentation of flipping the input sentences applied to the train dataloader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHerrK5LNJbG"
      },
      "source": [
        "### Utility functions\n",
        "\n",
        "We create a class to hold the dataset to be loaded into the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCEObxVMKUad"
      },
      "source": [
        "# We create a Dataset so we can create minibatches\n",
        "class Task2Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, train_data, labels):\n",
        "        self.x_train = train_data\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train[item], self.y_train[item]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD5PF9bnYsRk"
      },
      "source": [
        "### Creating Dataloaders\n",
        "\n",
        "The first function, flip_label, is used in the data augmentation process. Then the two following functions are used to create and return pytorch dataloaders for the training and test datasets. note that create_dataloaders splits the dataset into a training and validation set dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yXQMGWHW_ti"
      },
      "source": [
        "def flip_label(label):\n",
        "  if label == 1:\n",
        "    return 2\n",
        "  elif label == 2:\n",
        "    return 1\n",
        "  else: \n",
        "    return label\n",
        "\n",
        "def create_dataloaders(feature, labels, collate_fn, train_proportion=0.8, batch_size=32, flip=True, seed=1):\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
        "    train_and_dev = Task2Dataset(feature, labels)\n",
        "\n",
        "    train_examples = round(len(train_and_dev)*train_proportion)\n",
        "    dev_examples = len(train_and_dev) - train_examples\n",
        "\n",
        "    train_dataset, dev_dataset = random_split(train_and_dev,\n",
        "                                              (train_examples,\n",
        "                                                dev_examples))\n",
        "\n",
        "    if flip:\n",
        "        # Add the reverse examples to the training set to create more training data\n",
        "        flipped_train_dataset = []\n",
        "        for r in train_dataset:\n",
        "          flipped_train_dataset.append((list(reversed(r[0])), flip_label(r[1])))\n",
        "        train_dataset += flipped_train_dataset\n",
        "\n",
        "\n",
        "    # np.random.seed(SEED)\n",
        "    # np.random.shuffle(train_dataset)\n",
        "    # idx = torch.randperm(train_dataset.shape[0])\n",
        "    # train_dataset = train_dataset[idx].view(train_dataset.size()) \n",
        "      \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "    dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    \n",
        "    return train_loader, dev_loader\n",
        "\n",
        "def create_test_dataloader(feature, collate_fn, batch_size=32):\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(feature, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    \n",
        "    return test_loader\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvXJ20y4m6y4"
      },
      "source": [
        "# Approach 1: Using pre-trained representations\n",
        "\n",
        "In this section, we explore three different methods for learning which of the two edited sentences are funnier. We first explore the use of the GloVe embeddings with a BiLSTM, then we use BERT embeddings with the same model structure and finally, we use the entire BERT architecture, including the transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SiCAvnONQCO"
      },
      "source": [
        "## 1.1 Using pre-trained (GloVe) embeddings with a BiLSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FlQVqkEICEE"
      },
      "source": [
        "###Create vocab and load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szdwn4PS-9_Y",
        "outputId": "97fca91f-284c-429e-9f7d-beb85601d175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating word vectors\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
        "\n",
        "# Creating joint vocab from test and train\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data + test_data)\n",
        "\n",
        "print(\"Vocab created.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGQJ_QNYFu_p"
      },
      "source": [
        "# We create representations for our tokens\n",
        "wvecs = [] # word vectors\n",
        "\n",
        "vocab = Vocabulary()\n",
        "# This is a large file, it will take a while to load in the memory!\n",
        "with codecs.open('glove.6B.300d.txt', 'r','utf-8') as f:\n",
        "  index = 1\n",
        "  for line in f.readlines():\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in joint_vocab:\n",
        "          (word, vec) = (word,\n",
        "                     list(map(float,line.strip().split()[1:])))\n",
        "          wvecs.append(vec)\n",
        "          vocab.add_word(word)\n",
        "\n",
        "\n",
        "wvecs = np.array(wvecs)\n",
        "\n",
        "vectorized_seqs = [[[vocab.word2idx(tok) for tok in sen if tok in vocab._word2idx] for sen in seq] for seq in training_tokenized_corpus]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Q_rgNjIN0L"
      },
      "source": [
        "# Used for collating our observations into minibatches:\n",
        "def collate_fn_padd(batch, test=False):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "    if test:\n",
        "        batch_features = batch\n",
        "    else:\n",
        "        batch_labels = [l for f, l in batch]\n",
        "        batch_features = [f for f, l in batch]\n",
        "    \n",
        "    batch_features_len = [[len(s) for s in b] for b in batch_features]\n",
        "\n",
        "    seq_tensor = torch.zeros((2, len(batch_features), np.max(batch_features_len))).long()\n",
        "\n",
        "    # Shape of seq_tensor is batch_size x max_feature_len\n",
        "    # It should be batch_size x 2 x max_feature_len\n",
        "\n",
        "    for idx, (seq, seqlens) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        for i in range(2):\n",
        "            seq_tensor[i, idx, :seqlens[i]] = torch.LongTensor(seq[i])\n",
        "\n",
        "    if test:\n",
        "      return seq_tensor\n",
        "\n",
        "    batch_labels = torch.LongTensor(batch_labels)\n",
        "    return seq_tensor, batch_labels"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKwiK_pKQbe",
        "outputId": "88ac31cb-5fa5-478b-9c4b-efd26fa59b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "feature = vectorized_seqs\n",
        "\n",
        "train_loader, dev_loader = create_dataloaders(feature, labels, collate_fn_padd)\n",
        "\n",
        "print(\"Dataloaders created.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataloaders created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eraY3strIOrJ"
      },
      "source": [
        "### BiLSTM Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYyl1tpwm6y-"
      },
      "source": [
        "class BiLSTM_classification(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, device, num_layers=1, dropout_p=0.2):\n",
        "        super(BiLSTM_classification, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device=device\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.emb_dropout = nn.Dropout(dropout_p)\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True, dropout=dropout_p)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 4, 3)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedding1 = self.emb_dropout(self.embedding(sentence[0]))\n",
        "        embedding1 = embedding1.permute(1, 0, 2)\n",
        "\n",
        "        embedding2 = self.emb_dropout(self.embedding(sentence[1]))\n",
        "        embedding2 = embedding2.permute(1, 0, 2)\n",
        "\n",
        "        lstm_out_1, _ = self.lstm(embedding1.view(len(embedding1), -1, self.embedding_dim))\n",
        "        lstm_out_2, _ = self.lstm(embedding2.view(len(embedding2), -1, self.embedding_dim))\n",
        "        \n",
        "        # concat both lstm_out_1 and lstm_out_2 and give to our linear layer\n",
        "        # lstm_out_1[-1] are both batch_size x (2 * hidden_size)\n",
        "        # We concat so it's batch_size x (2*(2 * hidden_size))\n",
        "        out = self.hidden2label(torch.cat([lstm_out_1[-1], lstm_out_2[-1]], 1))\n",
        "        return out"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD_1RRbvIVPC"
      },
      "source": [
        "### Model Instantiation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJtG2gnyD5AU",
        "outputId": "f16dcaa0-23ff-4926-d72f-19fd75fc697f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 20\n",
        "\n",
        "INPUT_DIM = len(vocab) + 1\n",
        "HIDDEN_DIM = 200 \n",
        "EMBEDDING_DIM = 300\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "model = BiLSTM_classification(EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, device, num_layers, dropout)\n",
        "print(\"Model initialised.\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# We provide the model with our embeddings\n",
        "model.embedding.weight.data[1:].copy_(torch.from_numpy(wvecs))\n",
        "# Freeze model embeddings\n",
        "model.embedding.weight.requires_grad = False\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.96 | Train Accuracy: 0.47 |         Val. Loss: 0.95 | Val. Accuracy: 0.50 |\n",
            "| Epoch: 02 | Train Loss: 0.94 | Train Accuracy: 0.51 |         Val. Loss: 0.95 | Val. Accuracy: 0.50 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-69181af0605f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-2fe1991a5ec9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, dev_iter, model, number_epoch, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcL6evN-OkhI"
      },
      "source": [
        "### Obtain test set predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZksGlYFGCJaL"
      },
      "source": [
        "test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
        "vectorized_test = [[[vocab.word2idx(tok) for tok in sen if tok in vocab._word2idx] for sen in seq] for seq in test_tokenized_corpus]\n",
        "\n",
        "test_features = vectorized_test\n",
        "test_loader = create_test_dataloader(test_features, lambda b: collate_fn_padd(b, test=True))\n",
        "\n",
        "test_results = eval_test(test_loader, model, edited_test_df['id'], \"lstm\")\n",
        "\n",
        "test_results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Foh760W1IxgS"
      },
      "source": [
        "### Make box plots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--t-XGhBI8PS",
        "outputId": "c3759a67-762c-40e9-b8a8-543782006975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Three Experiments - Punctuation Removal, Stopword Removal and Sentence Flipping\n",
        "# This cell will take a long time to run, as it trains the model 30 times \n",
        "\n",
        "def get_accuracies(train_loader, dev_loader):\n",
        "\n",
        "  epochs = 20\n",
        "  INPUT_DIM = len(vocab) + 1\n",
        "  HIDDEN_DIM = 200 \n",
        "  EMBEDDING_DIM = 300\n",
        "  num_layers = 2\n",
        "  dropout = 0.3\n",
        "  train_accs = []\n",
        "  val_accs = []\n",
        "  max_train_accs = []\n",
        "  max_val_accs = []\n",
        "\n",
        "  for i in range(5):\n",
        "\n",
        "    model = BiLSTM_classification(EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, device, num_layers, dropout)\n",
        "    print(\"Model initialised.\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # We provide the model with our embeddings\n",
        "    model.embedding.weight.data[1:].copy_(torch.from_numpy(wvecs))\n",
        "    # Freeze model embeddings\n",
        "    model.embedding.weight.requires_grad = False\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    loss_fn = loss_fn.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    _, train_acc, _, val_acc = train(train_loader, dev_loader, model, epochs, optimizer)\n",
        "    train_accs.append(train_acc[-1])\n",
        "    val_accs.append(val_acc[-1])\n",
        "    max_train_accs.append(max(train_acc))\n",
        "    max_val_accs.append(max(val_acc))\n",
        "\n",
        "  return train_accs, val_accs, max_train_accs, max_val_accs\n",
        "\n",
        "# Baseline\n",
        "print(\"Running Baseline Tests\")\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data, remove_punc=0, remove_stopwords=0)\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data + test_data, remove_punc=0, remove_stopwords=0)\n",
        "train_loader, dev_loader = create_dataloaders(feature, labels, collate_fn_padd, flip=0)\n",
        "baseline_train_accs, baseline_val_accs, max_baseline_train_accs, max_baseline_val_accs = get_accuracies(train_loader, dev_loader)\n",
        "\n",
        "# Punctuation Removal\n",
        "print(\"Running Punctuation Removal Tests\")\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data, remove_punc=1, remove_stopwords=0)\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data + test_data, remove_punc=1, remove_stopwords=0)\n",
        "train_loader, dev_loader = create_dataloaders(feature, labels, collate_fn_padd, flip=0)\n",
        "punc_train_accs, punc_val_accs, max_punc_train_accs, max_punc_val_accs = get_accuracies(train_loader, dev_loader)\n",
        "\n",
        "# Stop Word Removal\n",
        "print(\"Running Stop Word Removal Tests\")\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data, remove_punc=0, remove_stopwords=1)\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data + test_data, remove_punc=0, remove_stopwords=1)\n",
        "train_loader, dev_loader = create_dataloaders(feature, labels, collate_fn_padd, flip=0)\n",
        "stop_train_accs, stop_val_accs, max_stop_train_accs, max_stop_val_accs = get_accuracies(train_loader, dev_loader)\n",
        "\n",
        "# Both Data Cleaning\n",
        "print(\"Running Both Data Cleaning Tests\")\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data, remove_punc=1, remove_stopwords=1)\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data + test_data, remove_punc=1, remove_stopwords=1)\n",
        "train_loader, dev_loader = create_dataloaders(feature, labels, collate_fn_padd, flip=0)\n",
        "clean_train_accs, clean_val_accs, max_clean_train_accs, max_clean_val_accs = get_accuracies(train_loader, dev_loader)\n",
        "\n",
        "# Data Flipping\n",
        "print(\"Running Data Flipping Tests\")\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data, remove_punc=1, remove_stopwords=1)\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data + test_data, remove_punc=1, remove_stopwords=1)\n",
        "train_loader, dev_loader = create_dataloaders(feature, labels, collate_fn_padd, flip=1)\n",
        "flip_train_accs, flip_val_accs, max_flip_train_accs, max_flip_val_accs = get_accuracies(train_loader, dev_loader)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.97 | Val. Accuracy: 0.45 |\n",
            "| Epoch: 02 | Train Loss: 0.94 | Train Accuracy: 0.51 |         Val. Loss: 0.96 | Val. Accuracy: 0.48 |\n",
            "| Epoch: 03 | Train Loss: 0.91 | Train Accuracy: 0.56 |         Val. Loss: 0.96 | Val. Accuracy: 0.51 |\n",
            "| Epoch: 04 | Train Loss: 0.82 | Train Accuracy: 0.62 |         Val. Loss: 0.94 | Val. Accuracy: 0.55 |\n",
            "| Epoch: 05 | Train Loss: 0.71 | Train Accuracy: 0.70 |         Val. Loss: 0.96 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 06 | Train Loss: 0.60 | Train Accuracy: 0.75 |         Val. Loss: 1.00 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 07 | Train Loss: 0.50 | Train Accuracy: 0.79 |         Val. Loss: 1.12 | Val. Accuracy: 0.52 |\n",
            "| Epoch: 08 | Train Loss: 0.42 | Train Accuracy: 0.83 |         Val. Loss: 1.15 | Val. Accuracy: 0.58 |\n",
            "| Epoch: 09 | Train Loss: 0.36 | Train Accuracy: 0.86 |         Val. Loss: 1.34 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 10 | Train Loss: 0.32 | Train Accuracy: 0.87 |         Val. Loss: 1.37 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 11 | Train Loss: 0.28 | Train Accuracy: 0.89 |         Val. Loss: 1.42 | Val. Accuracy: 0.58 |\n",
            "| Epoch: 12 | Train Loss: 0.24 | Train Accuracy: 0.91 |         Val. Loss: 1.61 | Val. Accuracy: 0.60 |\n",
            "| Epoch: 13 | Train Loss: 0.22 | Train Accuracy: 0.91 |         Val. Loss: 1.55 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 14 | Train Loss: 0.20 | Train Accuracy: 0.92 |         Val. Loss: 1.51 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 15 | Train Loss: 0.18 | Train Accuracy: 0.93 |         Val. Loss: 1.61 | Val. Accuracy: 0.62 |\n",
            "| Epoch: 16 | Train Loss: 0.17 | Train Accuracy: 0.94 |         Val. Loss: 1.66 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 17 | Train Loss: 0.16 | Train Accuracy: 0.94 |         Val. Loss: 1.71 | Val. Accuracy: 0.60 |\n",
            "| Epoch: 18 | Train Loss: 0.15 | Train Accuracy: 0.94 |         Val. Loss: 1.71 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 19 | Train Loss: 0.14 | Train Accuracy: 0.95 |         Val. Loss: 1.64 | Val. Accuracy: 0.62 |\n",
            "| Epoch: 20 | Train Loss: 0.13 | Train Accuracy: 0.95 |         Val. Loss: 1.82 | Val. Accuracy: 0.61 |\n",
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.96 | Train Accuracy: 0.46 |         Val. Loss: 0.95 | Val. Accuracy: 0.48 |\n",
            "| Epoch: 02 | Train Loss: 0.95 | Train Accuracy: 0.49 |         Val. Loss: 0.95 | Val. Accuracy: 0.50 |\n",
            "| Epoch: 03 | Train Loss: 0.92 | Train Accuracy: 0.54 |         Val. Loss: 0.94 | Val. Accuracy: 0.53 |\n",
            "| Epoch: 04 | Train Loss: 0.85 | Train Accuracy: 0.60 |         Val. Loss: 0.94 | Val. Accuracy: 0.54 |\n",
            "| Epoch: 05 | Train Loss: 0.75 | Train Accuracy: 0.67 |         Val. Loss: 0.95 | Val. Accuracy: 0.57 |\n",
            "| Epoch: 06 | Train Loss: 0.63 | Train Accuracy: 0.74 |         Val. Loss: 0.99 | Val. Accuracy: 0.58 |\n",
            "| Epoch: 07 | Train Loss: 0.54 | Train Accuracy: 0.78 |         Val. Loss: 1.08 | Val. Accuracy: 0.57 |\n",
            "| Epoch: 08 | Train Loss: 0.45 | Train Accuracy: 0.82 |         Val. Loss: 1.14 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 09 | Train Loss: 0.39 | Train Accuracy: 0.84 |         Val. Loss: 1.24 | Val. Accuracy: 0.56 |\n",
            "| Epoch: 10 | Train Loss: 0.34 | Train Accuracy: 0.86 |         Val. Loss: 1.22 | Val. Accuracy: 0.60 |\n",
            "| Epoch: 11 | Train Loss: 0.29 | Train Accuracy: 0.88 |         Val. Loss: 1.48 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 12 | Train Loss: 0.26 | Train Accuracy: 0.90 |         Val. Loss: 1.45 | Val. Accuracy: 0.57 |\n",
            "| Epoch: 13 | Train Loss: 0.23 | Train Accuracy: 0.91 |         Val. Loss: 1.44 | Val. Accuracy: 0.60 |\n",
            "| Epoch: 14 | Train Loss: 0.21 | Train Accuracy: 0.92 |         Val. Loss: 1.66 | Val. Accuracy: 0.60 |\n",
            "| Epoch: 15 | Train Loss: 0.19 | Train Accuracy: 0.93 |         Val. Loss: 1.61 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 16 | Train Loss: 0.18 | Train Accuracy: 0.93 |         Val. Loss: 1.70 | Val. Accuracy: 0.60 |\n",
            "| Epoch: 17 | Train Loss: 0.16 | Train Accuracy: 0.94 |         Val. Loss: 1.76 | Val. Accuracy: 0.60 |\n",
            "| Epoch: 18 | Train Loss: 0.16 | Train Accuracy: 0.94 |         Val. Loss: 1.69 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 19 | Train Loss: 0.14 | Train Accuracy: 0.95 |         Val. Loss: 1.86 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 20 | Train Loss: 0.13 | Train Accuracy: 0.95 |         Val. Loss: 1.82 | Val. Accuracy: 0.62 |\n",
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.96 | Train Accuracy: 0.46 |         Val. Loss: 0.95 | Val. Accuracy: 0.49 |\n",
            "| Epoch: 02 | Train Loss: 0.94 | Train Accuracy: 0.51 |         Val. Loss: 0.95 | Val. Accuracy: 0.49 |\n",
            "| Epoch: 03 | Train Loss: 0.91 | Train Accuracy: 0.55 |         Val. Loss: 0.96 | Val. Accuracy: 0.50 |\n",
            "| Epoch: 04 | Train Loss: 0.84 | Train Accuracy: 0.62 |         Val. Loss: 0.95 | Val. Accuracy: 0.54 |\n",
            "| Epoch: 05 | Train Loss: 0.72 | Train Accuracy: 0.69 |         Val. Loss: 0.90 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 06 | Train Loss: 0.61 | Train Accuracy: 0.74 |         Val. Loss: 0.97 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 07 | Train Loss: 0.51 | Train Accuracy: 0.79 |         Val. Loss: 1.09 | Val. Accuracy: 0.58 |\n",
            "| Epoch: 08 | Train Loss: 0.42 | Train Accuracy: 0.83 |         Val. Loss: 1.11 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 09 | Train Loss: 0.36 | Train Accuracy: 0.85 |         Val. Loss: 1.24 | Val. Accuracy: 0.59 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-ffea73539ebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mjoint_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_tokenized_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_padd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mstop_train_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_val_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_stop_train_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_stop_val_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mtraining_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_tokenized_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-ffea73539ebc>\u001b[0m in \u001b[0;36mget_accuracies\u001b[0;34m(train_loader, dev_loader)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mval_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-2fe1991a5ec9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, dev_iter, model, number_epoch, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV_v2t_oLltR",
        "outputId": "f28c528d-06eb-4f30-dcec-b337dce60da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Plot box plots!\n",
        "data = {\"No Data Cleaning\": baseline_train_accs, \"Punctuation Removal\": punc_train_accs, \"Stop Word Removal\": stop_train_accs, \"Both Data Cleaning\": clean_train_accs}\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.set_title('Traning Accuracy For Data Cleaning')\n",
        "ax1.boxplot(data.values())\n",
        "ax1.set_xticklabels(data.keys())\n",
        "plt.savefig('Data Cleaning Training.pdf', format='pdf')\n",
        "plt.show()\n",
        "\n",
        "data = {\"No Data Cleaning\": baseline_valid_accs, \"Punctuation Removal\": punc_valid_accs, \"Stop Word Removal\": stop_valid_accs, \"Both Data Cleaning\": clean_valid_accs}\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.set_title('Validation Accuracy For Data Cleaning')\n",
        "ax1.boxplot(data.values())\n",
        "ax1.set_xticklabels(data.keys())\n",
        "plt.savefig('Data Cleaning Validation.pdf', format='pdf')\n",
        "plt.show()\n",
        "\n",
        "data = {\"No Data Augmentation\": clean_train_accs, \"Data Augmentation\": flip_train_accs}\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.set_title('Training Accuracy For Data Augmentation')\n",
        "ax1.boxplot(data.values())\n",
        "ax1.set_xticklabels(data.keys())\n",
        "plt.savefig('Data Augmentation Training.pdf', format='pdf')\n",
        "plt.show()\n",
        "\n",
        "data = {\"No Data Augmentation\": clean_valid_accs, \"Data Augmentation\": flip_valid_accs}\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.set_title('Validation Accuracy For Data Augmentation')\n",
        "ax1.boxplot(data.values())\n",
        "ax1.set_xticklabels(data.keys())\n",
        "plt.savefig('Data Augmentation Validation.pdf', format='pdf')\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAEICAYAAADiGKj0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAb7UlEQVR4nO3df7zmZV3n8ddbBtBCYJBR+T1uDPkjlWpCyzIqqdFS3Nrlh5uIBTzKdVvXsoUwRDKt3basdrLEXEREMkubTYxHPtQsFozBCJ0hYACLARQYBlBJgfjsH99r8MvhnDn3DPdwrjnzej4e9+Pc9/W97ut7fX/c13nf3x/npKqQJElSv56w0B2QJEnS1hnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYNuFJFmX5OiF7kePklSSw3dAu19M8pL2/FeSvGeSutsxnx9Icu329lNaSI5Nc3Ns0hY7JLC1jXt/kv1nlP9D2/mWT3l+y1u7F88ovyDJ2Y+hva+2xxeTnD61Ds8+z7OTXDDF9s5L8rZxWVU9p6o+Pa15jOb16SRfb+vqziR/nuSAac9nIST5wyTnz1L+/CTfSLLfpG1V1dur6pQp9esRg3hV/W1Vffs02p5jfnu17fvxHTWPndECjnWOTZPNy7FpAo5NO4cdeYTtJuDELS+SPBf4lh04P4AXJPm+Kba3b1XtxbAcZyVZNcW2F5vXt3V1OLAX8FsL3J9peR/wk0m+dUb5q4G/rKq7FqBPC+GngG8AxyR5+uM54yRLHs/5bYeFGOscmybn2LS47TpjU1VN/QF8EXgzcMWo7LeAM4EClreyHwf+AbgXuBk4e1T/eIaBcO/2+qXAl4Bls8xveWv3vwOfGpVfMKPNU4ENwF3AGuDAOfq/pb0lo7IrgF8CTgb+bkb9Ag5vz88DVgMfA74CfBb4tlHd5wB/3frwZeBXgFXA/cADwFeBfxytx5eM3ns2cMHo9Z+2dXIP8BngOa38tNbW/a29/zuzPWBP4J3Are3xTmDPNu1oYCPwi8DtwG3Aa7eyvT8NnDJ6/Tpg3ej1M0fLfC1w3GjaecAfAB9vfb0UeHrrz2bgn4DvHNV/Vpvf3cA64BWt/AVtXew2qvvvgavb86OAy9r7bgP+N7DHbNtwluW7Fjhp9Hq3ts6OBb4N+CSwCbgT+ADDL1NmWeczt9+rgX9u7z1zRt05+9u2dQFfa+vs+C3bbL71NMk+Osc6+CTw68DngF+aMe37gf/X5nUzcHIrfxLwv9oy3gP8XSt7RF/nWE8fZvj83gucMsH2m+1z9XTgPuApo3rfBdwB7L6Tj3WOTY5N4NgEu9DY9JgHrK0MYi9pO9Oz2k60ETiMRw5iRwPPZTjS97y2MK8ctfOBtgGfwrAT/sQc81ve2n0ycMto5T4c2IAfZthpv4thQPh94DPztLcECPCitnJ/hMkGxU1tIy5py3BRm/bktkF/EXhie/2C2T4wEw6KP9Pa2DLAXTVjx3/bVna8c4DLgacCyxh26l8bbZcHW53dgZe15V8636DYttUngL9or7+V4YPy2rY+vrNth2eP+nkn8N1tnXyS4ZfXSQz7zdtoIbz1ZQPDDr9H26ZfAb69Tb8BOGbUrz8FTm/Pvxt4YevDcuAa4A0TDopnAp8Yvf4x2geL4Vv7MW0bLGMYsN4536AIPJthQHtxe+9vt3X+ku3pL6OBZoL1dB5z7KNzLP9hwEOtz79I+0UzmvYVhiM9u7ftf2SbtrrtGwe1bfl9bVkf7utW1tMDwCsZxoYnbW19sPXP1cXAz4/m8zvA7y+Csc6xybEJHJt2qbFpRwe2NwPvYPiW9tdtgR4exGZ53zuB3xm93hf4F+DzwB9tZX7L+eYg9jrg8lY+Dmx/DPyP0Xv2aiv+UX0ZtXc3wzepa4BfaNNOZv5B8T2jaS8D/qk9PxH4hzmW4Wy2cVCcUXff1o99Rv3Y2qB4A/CyGR/0L44+YP/KI7/F3w68cI55f5ph0Lyn9eEq4NA27Xjgb2fU/yPgLaN+njua9l+Aa0avnwvc3Z7/AMM31SeMpn9wtI3fBrx39EH5GnDYHH1+A/CR2bbhLHUPbfvKwe31B4DfnaPuK8fbmLkHxbMYDUQMvzzuH2/vbekvjxwU51tP5zHHPjrHvN9M+4XLMMD9G+3IAnDGuF+j9zyh7UPPn2Xaw33dynqa9cvUbOuDrX+ujgcubc93a+vlqK21vS0PFm6sc2z65nTHJscm2AXGph19l+j7gVcxDCSzXRz5giSfSnJHknuAnwMevni3qu5m+CbyHQyHLyfxHuBpSV4+o/xAhsOfW9r+KkOSP2grbe1fVUur6llV9XsTzh+GFb/FfQzhEOAQhsHoMUuyW5LfSHJDknsZdioYrb95PGJ9tOcHjl5vqqoHR6/HyzGbX6iqfRiOHiwFDm7lhzFcW3j3lgfwnxgOCW/x5dHzf53l9Zb5HgjcXFUPzej3lm14IcM1HXsCPwl8rqr+GSDJEUn+MsmX2vp6OxOuq6r6F4Zvpz+dZC+Gge/81u7TklyU5JbW7gUTtnsgw7f7LfP4GsP+yGPtL/OvJ5h7H53NSQy/CKiqW4C/AV7Tps21T+/P8I1ye/f3m8cv5lkfW/tc/QXw7CTPYDjacE9V/f129mlrHu+xzrHpmxybHJt2ibFphwa2tkPexJCS/3yWKhcyXEt2SPtA/SHDYX4AkhzJcGj9g8BEg1JV3Q+8Ffi1cVsMpxkOG7X9rQyHSG+ZfImA4ZvRwxcUb+NFjjcD/26OaTXfvHjkQPIqhusUXgLsw/DNG765zLO1N/aI9cHwTe3Wed4zr6r6PMO3ydVJwrDMf1NV+44ee1XVz29H87cChyQZ77eH0rZhVa1n+PC/lGH9XDiq9y6Ga05WVNXeDIfkx/vHfN7HcF3HTwE3VdWVrfztDOv6ua3dn56w3dsYPswAJPkWhv1xGv3d6nraFu0mnhXAGW1A+hLDNTmvahfc3sxwrcxMdwJfn2PazM/QbgynbMZm7r9bWx9zfq6q6uvAhxi2y6sZgtXULcRYNwvHpq1wbHJsanbasenx+DtsPwv8cEvpMz0ZuKuqvp7kKIYdGYAkT2T4RvArDNcYHJTkdRPO8/0MCXp859QHgdcmObJ9y3k78Nmq+uI2Ls8/As9p7TyR4RDppP4SOCDJG5LsmeTJSV7Qpn0ZWD5jR74KOCHJ7klWAv9hNO3JDHfGbGLYwd4+Y15fZu4BGIb18eYkyzL8SYKzGNb3NLwPeBrwCoZlPiLJq9ty7J7ke5I8azva/SzDN65fbu0cDbwcuGhU50LgvzJcf/Gno/InM1wk+tUkzwS2dVD+M4aB5a0Myzdu96vAPUkOAt40YXsfBn4iyfcn2YPhmpzxtp+vv1vbvpOsp0m9huEU37OBI9vjOxiu3Xgpw7fblyQ5LsmSJE9JcmT7Bv1e4LeTHNiOunxv++xdBzwxyY8n2Z3htMae8/Rja+tja58rGI44nMywP+6QwNYsxFg35tg0P8em+Tk2dTo27fDAVlU3VNXaOSa/DjgnyVcYPpQfGk17B8Oh03dV1TcYUujbkqyYYJ7/1trbb1T2CeBXGXbu2xjS9QnbsTzXMezAnwCuZ7i7ZNL3foXh0OfLGQ77Xg/8UJu85QO8Kcnn2vNfbf3czPBhHH8rO5/hG9stwHqGi3TH/pjhcOvdST46S3feBqwFrma4buZzrewxa0c5fxf41bbMP8qwrm9lWO7fZP4PwVztvpzhw3gnwx1cJ1XVP42qfRD4QeCTVXXnqPyXGH5JfgU4F/iTbZz31xj2nYNph+CbtzLcyHIPw51Nsx1dma29dcB/ZtimtzFs443b0N+zgfe17XvcjLYnWU/zar/0j2O4EPZLo8dNDIPLa9opmZcxXFR7F8Mv8uePluHzDHcx3sWw3Z9QVfcwfPbfw7D/fm3Gss9mzvUxz+eKqrqU4cLkh09D7QgLMdbNmL9j0zwcmyZqz7Gp07Ep7YI3SVq0knwSuLCq5vxr7pL0eNuWscnAJmlRS/I9DKdODmnfeCVpwW3r2OT/EpW0aCV5H8MpwjcY1iT1YnvGJo+wSZIkdc4jbJIkSZ3r/Z8qP8L+++9fy5cvX+huSHqcXHnllXdW1cy/g7RTcvySdj3THMN2qsC2fPly1q6d6655SYtNkh32Zzgeb45f0q5nmmOYp0QlSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6txEgS3JqiTXJtmQ5PQ56hyXZH2SdUkubGWHJflckqta+c+N6n93ks+3Nn8vSaazSJIkSYvLvH+HLcluwGrgGGAjcEWSNVW1flRnBXAG8KKq2pzkqW3SbcD3VtU3kuwFfKG991bgXcCpwGeBi4FVwMenuGySJEmLwiRH2I4CNlTVjVV1P3ARcOyMOqcCq6tqM0BV3d5+3l9V32h19twyvyQHAHtX1eU1/DPT84FXPualkSTpcZJk6g9pLpMEtoOAm0evN7aysSOAI5JcmuTyJKu2TEhySJKrWxu/2Y6uHdTa2VqbkiR1q6omemxrXWk207rpYAmwAjgaOBE4N8m+AFV1c1U9DzgceE2Sp21Lw0lOS7I2ydo77rhjSt2VpB3P8UvStEwS2G4BDhm9PriVjW0E1lTVA1V1E3AdQ4B7WDuy9gXgB9r7D56nzS3ve3dVrayqlcuWLYr/AS1pF+H4JWlaJglsVwArkjwjyR7ACcCaGXU+ynB0jST7M5wivTHJwUme1MqXAt8PXFtVtwH3Jnlhuzv0JOAvprFAkiRJi828ga2qHgReD1wCXAN8qKrWJTknyStatUuATUnWA58C3lRVm4BnAZ9N8o/A3wC/VVWfb+95HfAeYANwA94hKkmSNKt5/6wHQFVdzPCnN8ZlZ42eF/DG9hjX+WvgeXO0uRb4jm3sryRJ0i7H/3QgSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdm+h/iUqStCvZb7/92Lx589TaSzKVdpYuXcpdd901lba0czGwSZI0w+bNm6mqhe7Go0wr+Gnn4ylRSZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6tyShe6AJEm9qbfsDWfvs9DdeJR6y94L3QUtEAObJEkz5K33UlUL3Y1HSUKdvdC90ELwlKgkSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5yYKbElWJbk2yYYkp89R57gk65OsS3JhKzsyyWWt7Ookx4/q/0iSzyW5KsnfJTl8OoskSZK0uMz7Zz2S7AasBo4BNgJXJFlTVetHdVYAZwAvqqrNSZ7aJt0HnFRV1yc5ELgyySVVdTfwLuDYqromyeuANwMnT3PhJEmSFoNJjrAdBWyoqhur6n7gIuDYGXVOBVZX1WaAqrq9/byuqq5vz28FbgeWtfcUsOUvAO4D3PpYFkSSJGmxmuQP5x4E3Dx6vRF4wYw6RwAkuRTYDTi7qv5qXCHJUcAewA2t6BTg4iT/CtwLvHC2mSc5DTgN4NBDD52gu5LUB8cvSdMyrZsOlgArgKOBE4Fzk+y7ZWKSA4D3A6+tqoda8X8DXlZVBwP/B/jt2RquqndX1cqqWrls2bLZqkhSlxy/JE3LJIHtFuCQ0euDW9nYRmBNVT1QVTcB1zEEOJLsDXwMOLOqLm9ly4DnV9Vn2/v/BPi+7V4KSZKkRWySwHYFsCLJM5LsAZwArJlR56MMR9dIsj/DKdIbW/2PAOdX1YdH9TcD+yQ5or0+Brhmu5dCkiRpEZv3GraqejDJ64FLGK5Pe29VrUtyDrC2qta0aT+aZD3wb8CbqmpTkp8GXgw8JcnJrcmTq+qqJKcCf5bkIYYA9zNTXzpJkqRFIFW10H2Y2MqVK2vt2rUL3Q1Jj5MkV1bVyoXuxzQ4fu1cktDj78de+6XZTXMM8z8dSJIkdc7AJkmS1LlJ/g6bJEm7nCQL3YVHWbp06UJ3QQvEwCZJ0gzTvE7M6840DZ4SlSRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4tWegOSJK0M0oy9bpVtb3d0SJnYJMkaTsYrvR48pSoJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnZsosCVZleTaJBuSnD5HneOSrE+yLsmFrezIJJe1squTHD+qnyS/nuS6JNck+YXpLJIkSdLismS+Ckl2A1YDxwAbgSuSrKmq9aM6K4AzgBdV1eYkT22T7gNOqqrrkxwIXJnkkqq6GzgZOAR4ZlU9NHqPJEmSRuYNbMBRwIaquhEgyUXAscD6UZ1TgdVVtRmgqm5vP6/bUqGqbk1yO7AMuBv4eeBVVfXQ+D2SJEl6pElOiR4E3Dx6vbGVjR0BHJHk0iSXJ1k1s5EkRwF7ADe0om8Djk+yNsnH21G6R0lyWquz9o477pigu5LUB8cvSdMyrZsOlgArgKOBE4Fzk+y7ZWKSA4D3A6/dckQN2BP4elWtBM4F3jtbw1X17qpaWVUrly1bNqXuStKO5/glaVomCWy3MFxrtsXBrWxsI7Cmqh6oqpuA6xgCHEn2Bj4GnFlVl894z5+35x8Bnrft3ZckSVr8JglsVwArkjwjyR7ACcCaGXU+ynB0jST7M5wivbHV/whwflV9eJb3/FB7/oMMIU+SJEkzzBvYqupB4PXAJcA1wIeqal2Sc5K8olW7BNiUZD3wKeBNVbUJOA54MXBykqva48j2nt8AfirJ54F3AKdMdckkSZIWiUnuEqWqLgYunlF21uh5AW9sj3GdC4AL5mjzbuDHt7G/kiRJuxz/04EkSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ2bKLAlWZXk2iQbkpw+R53jkqxPsi7Jha3syCSXtbKrkxw/y/t+L8lXH9tiSJIkLV5L5quQZDdgNXAMsBG4Ismaqlo/qrMCOAN4UVVtTvLUNuk+4KSquj7JgcCVSS6pqrvb+1YCS6e7SJIkSYvLJEfYjgI2VNWNVXU/cBFw7Iw6pwKrq2ozQFXd3n5eV1XXt+e3ArcDy+DhIPg/gV+exoJIkiQtVpMEtoOAm0evN7aysSOAI5JcmuTyJKtmNpLkKGAP4IZW9HpgTVXdtu3dliRJ2nXMe0p0G9pZARwNHAx8JslzR6c+DwDeD7ymqh5qp0f/Y6u/VUlOA04DOPTQQ6fUXUna8Ry/JE3LJEfYbgEOGb0+uJWNbWQ4WvZAVd0EXMcQ4EiyN/Ax4MyqurzV/07gcGBDki8C35Jkw2wzr6p3V9XKqlq5bNmyCRdLkhae45ekaZkksF0BrEjyjCR7ACcAa2bU+SjtaFmS/RlOkd7Y6n8EOL+qPrylclV9rKqeXlXLq2o5cF9VHf6Yl0aSJGkRmjewVdWDDNebXQJcA3yoqtYlOSfJK1q1S4BNSdYDnwLeVFWbgOOAFwMnJ7mqPY7cIUsiSZK0SE10DVtVXQxcPKPsrNHzAt7YHuM6FwAXTND+XpP0Q5IkaVfkfzqQJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOTRTYkqxKcm2SDUlOn6POcUnWJ1mX5MJWdmSSy1rZ1UmOH9X/QGvzC0nem2T36SySJEnS4jJvYEuyG7AaeCnwbODEJM+eUWcFcAbwoqp6DvCGNuk+4KRWtgp4Z5J927QPAM8Engs8CTjlsS+OJEnS4rNkgjpHARuq6kaAJBcBxwLrR3VOBVZX1WaAqrq9/bxuS4WqujXJ7cAy4O6qunjLtCR/Dxz8GJdFO6EkU22vqqbaniRJPZjklOhBwM2j1xtb2dgRwBFJLk1yeZJVMxtJchSwB3DDjPLdgVcDfzXbzJOclmRtkrV33HHHBN3VzqSqJnpMWlfqieOXpGmZ1k0HS4AVwNHAicC5o1OfJDkAeD/w2qp6aMZ7/wD4TFX97WwNV9W7q2plVa1ctmzZlLorSTue45ekaZkksN0CHDJ6fXArG9sIrKmqB6rqJuA6hgBHkr2BjwFnVtXl4zcleQvDKdI3bl/3JUmSFr9JAtsVwIokz0iyB3ACsGZGnY8yHF0jyf4Mp0hvbPU/ApxfVR8evyHJKcCPASfOctRNkiRJzbyBraoeBF4PXAJcA3yoqtYlOSfJK1q1S4BNSdYDnwLeVFWbgOOAFwMnJ7mqPY5s7/lD4GnAZa38rOkumhbafvvtR5KpPICptbXffvst8JqRJGnbTHKXKO2OzotnlJ01el4MpzXfOKPOBcAFc7Q50by189q8eXOXNwJM+85USZJ2NP/TgSRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5L/zXDlNv2RvO3mehu/Eo9Za9F7oLkiRtEwObdpi89d5u7xKtsxe6F5IkTc5TopIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnvOlAO1SP/wZq6dKlC90FSZK2iYFNO8w07xBN0uUdp5IkPR48JSpJktQ5A5skSVLnDGySJEmdM7BJkiR1zpsOtKC25S7SSep6Y4IkaTEysGlBGbAkSZqfp0QlSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM5lZ/rDpUnuAP55ofuhBbE/cOdCd0KPu8OqatlCd2IaHL92aY5fu66pjWE7VWDTrivJ2qpaudD9kKRt5filafCUqCRJUucMbJIkSZ0zsGln8e6F7oAkbSfHLz1mXsMmSZLUOY+wSZIkdc7AJkmS1DkDm7qW5L1Jbk/yhYXuiyRtC8cvTZOBTb07D1i10J2QpO1wHo5fmhIDm7pWVZ8B7lrofkjStnL80jQZ2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdjUtSQfBC4Dvj3JxiQ/u9B9kqRJOH5pmvzXVJIkSZ3zCJskSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOwSZIkde7/AxjBNk/7njknAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX1hPU1F1liM"
      },
      "source": [
        "## 1.2 Using DistilBERT embeddings\n",
        "\n",
        "In this section, we will tokenize the sentences using DistilBERT's tokenizer and load the embeddings into a BiLSTM model of the same structure used in the previous section.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQPL8jfTquMW"
      },
      "source": [
        "### Tokenize sentences and load into dataloaders "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FupE9L_zIWWc"
      },
      "source": [
        "# Note that the following ones add an attention mask (used to tell BERT to ignore padding)\n",
        "def collate_fn_padd_BERT_embeddings(batch, test=False):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "    if test:\n",
        "        batch_features = batch\n",
        "    else: \n",
        "        batch_labels = [l for f, l in batch]\n",
        "        batch_features = [f for f, l in batch]\n",
        "\n",
        "    batch_features_len = [[len(s) for s in b] for b in batch_features]\n",
        "\n",
        "    seq_tensor = torch.zeros((2, len(batch), np.max(batch_features_len))).long()\n",
        "    mask_tensor = torch.zeros((2, len(batch), np.max(batch_features_len))).long()\n",
        "\n",
        "    # Shape of seq_tensor is batch_size x max_feature_len\n",
        "    # It should be batch_size x 2 x max_feature_len\n",
        "\n",
        "    for idx, (seq, seqlens) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        for i in range(2):\n",
        "            seq_tensor[i, idx, :seqlens[i]] = torch.LongTensor(seq[i])\n",
        "            mask_tensor[i, idx, :seqlens[i]] = torch.ones(seqlens[i])\n",
        "\n",
        "    feature_tensor = torch.stack([seq_tensor, mask_tensor], dim=1)\n",
        "\n",
        "    if test:\n",
        "        return feature_tensor\n",
        "\n",
        "    batch_labels = torch.LongTensor(batch_labels)\n",
        "    return feature_tensor, batch_labels"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTIMdGWn4-AB",
        "outputId": "a7947212-efd2-4022-d5c7-ca734bb0f8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "30d87e5c143b471faed6cad1481cce8a",
            "638acc671d5a4f17a7b358af73dd9c46",
            "5f64181564494ee492374a52c4a209e0",
            "0393cfde8905430596922a6b7134885d",
            "98873c8f22fe463d9bda3cf979d8b8a3",
            "95ec8ef9d8004dcd9ae78b9bb5214f2a",
            "a5a62ef1f2c940d785ccf127cb65bc25",
            "04674a47dd7b4ff69ca202a35aa75c76"
          ]
        }
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "input_id_pairs = []\n",
        "\n",
        "for (sentence1, sentence2) in training_data:\n",
        "    encoded_sentence1 = tokenizer.encode(sentence1)\n",
        "    encoded_sentence2 = tokenizer.encode(sentence2)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_id_pairs.append([encoded_sentence1, encoded_sentence2])\n",
        "\n",
        "train_loader, dev_loader = create_dataloaders(input_id_pairs, labels, collate_fn_padd_BERT_embeddings)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30d87e5c143b471faed6cad1481cce8a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b4T4VhqtS5"
      },
      "source": [
        "### BiLSTM model with DistilBERT embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6O_qyV_QpX"
      },
      "source": [
        "class BiLSTM_BERT_classification(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim, num_layers=1):\n",
        "        super(BiLSTM_BERT_classification, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased',\n",
        "            output_hidden_states = True, \n",
        "        )\n",
        "        # The word embedding dimension given by concatenating last 4 hidden \n",
        "        # layers of BERT\n",
        "        self.embedding_dim = 3072\n",
        "\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim, num_layers, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 4, 3)\n",
        "\n",
        "    def compute_embeddings(self, feature):\n",
        "        b_input_ids = feature[0].to(device)\n",
        "        b_input_mask = feature[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # We aren't trying to train BERT here, only use its embeddings\n",
        "            self.bert.eval()\n",
        "            outputs = self.bert(\n",
        "                b_input_ids,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "            hidden_states = outputs.hidden_states\n",
        "        \n",
        "        return torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        embedding1 = self.compute_embeddings(sentences[0])\n",
        "        embedding1 = embedding1.permute(1, 0, 2)\n",
        "        embedding2 = self.compute_embeddings(sentences[1])\n",
        "        embedding2 = embedding2.permute(1, 0, 2)\n",
        "\n",
        "        lstm_out_1, _ = self.lstm(embedding1.view(len(embedding1), -1, self.embedding_dim))\n",
        "        lstm_out_2, _ = self.lstm(embedding2.view(len(embedding2), -1, self.embedding_dim))\n",
        "        # concat both lstm_out_1 and lstm_out_2 and give to our linear layer, think we need\n",
        "        # output shape of lstm_ out is 2 * hidden_size as we do it in both directions\n",
        " \n",
        "        # lstm_out_1[-1] are both batch_size x (2 * hidden_size)\n",
        "        # do we concat so it's (2*batch_size) x (2 * hidden_size) or batch_size x (2*(2 * hidden_size))? - I chose the latter\n",
        "        out = self.hidden2label(torch.cat((lstm_out_1[-1], lstm_out_2[-1]), 1))\n",
        "        return out"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2P3zLeqrCLR"
      },
      "source": [
        "### Model Instantiation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnHthd2vRugs",
        "outputId": "eea973cc-97d0-4430-a57f-3d591140b466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829,
          "referenced_widgets": [
            "9b80e3f625304638962dd714364ab95b",
            "4e188ff22a564ed894a053ef7d2b81db",
            "4862cbe82b7f462fa28019e18888e99c",
            "2963ea50895d4ec096d9fa3a8af5a6f3",
            "9d65c6b99ff64a2ba38ae42c534a9064",
            "488fced3f5754de08c9c1327170c6a35",
            "ab61266a4ea24569a57f3717927b5f0b",
            "dc2ed7f4b33d47f68430950a3638fcbb",
            "07176c25d9b8461f91ea8e3dbb23b9d9",
            "72677f2f3550439a80c31911ab59531c",
            "faf2b48f606946bc948d7bceb778f079",
            "d7487957e8274f15b955c0d0507574f6",
            "961a6ffc22d04c07b0ddc3e4b8f3d691",
            "bb1900d1787e4a258cc29fcbf955131a",
            "efcedf053bc4452985033d95a356abea",
            "257f83a2c2914f65a5fb4bf28ffac4d5"
          ]
        }
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 8\n",
        "\n",
        "HIDDEN_DIM = 400 # Higher appears to yield better val acc\n",
        "num_layers = 4\n",
        "\n",
        "## TODO: Figure out network dimensions\n",
        "model = BiLSTM_BERT_classification(HIDDEN_DIM, num_layers)\n",
        "print(\"Model initialised.\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b80e3f625304638962dd714364ab95b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07176c25d9b8461f91ea8e3dbb23b9d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.97 | Train Accuracy: 0.44 |         Val. Loss: 0.96 | Val. Accuracy: 0.45 |\n",
            "| Epoch: 02 | Train Loss: 0.95 | Train Accuracy: 0.49 |         Val. Loss: 0.94 | Val. Accuracy: 0.53 |\n",
            "| Epoch: 03 | Train Loss: 0.90 | Train Accuracy: 0.58 |         Val. Loss: 0.91 | Val. Accuracy: 0.56 |\n",
            "| Epoch: 04 | Train Loss: 0.80 | Train Accuracy: 0.66 |         Val. Loss: 0.93 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 05 | Train Loss: 0.66 | Train Accuracy: 0.74 |         Val. Loss: 1.01 | Val. Accuracy: 0.61 |\n",
            "| Epoch: 06 | Train Loss: 0.52 | Train Accuracy: 0.81 |         Val. Loss: 1.27 | Val. Accuracy: 0.62 |\n",
            "| Epoch: 07 | Train Loss: 0.41 | Train Accuracy: 0.85 |         Val. Loss: 1.37 | Val. Accuracy: 0.63 |\n",
            "| Epoch: 08 | Train Loss: 0.34 | Train Accuracy: 0.87 |         Val. Loss: 1.58 | Val. Accuracy: 0.62 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.9659782733860054,\n",
              "  0.9537144392351561,\n",
              "  0.9010988997269439,\n",
              "  0.8007862808941048,\n",
              "  0.6647316114891377,\n",
              "  0.5209818066000382,\n",
              "  0.40716729400794877,\n",
              "  0.3380095958848702],\n",
              " [0.442238507661559,\n",
              "  0.487475016655563,\n",
              "  0.5754163890739507,\n",
              "  0.6574950033311125,\n",
              "  0.7419720186542305,\n",
              "  0.8070619586942038,\n",
              "  0.8457028647568288,\n",
              "  0.8696202531645569],\n",
              " [0.9575647219920209,\n",
              "  0.9393855516336111,\n",
              "  0.9080437627682554,\n",
              "  0.926219058697666,\n",
              "  1.006913043796889,\n",
              "  1.2663562058894111,\n",
              "  1.3719583045699195,\n",
              "  1.5777956636221424],\n",
              " [0.45149253731343286,\n",
              "  0.5282515991471215,\n",
              "  0.5628997867803838,\n",
              "  0.6050106609808102,\n",
              "  0.6124733475479744,\n",
              "  0.6183368869936035,\n",
              "  0.6268656716417911,\n",
              "  0.6242004264392325])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sul58j7jrUKt"
      },
      "source": [
        "### Obtain test set predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2oL9RyRNRV-"
      },
      "source": [
        "test_id_pairs = []\n",
        "\n",
        "for (sentence1, sentence2) in test_data:\n",
        "    encoded_sentence1 = tokenizer.encode(sentence1)\n",
        "    encoded_sentence2 = tokenizer.encode(sentence2)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    test_id_pairs.append([encoded_sentence1, encoded_sentence2])\n",
        "\n",
        "test_loader = create_test_dataloader(test_id_pairs, lambda b: collate_fn_padd_BERT_embeddings(b, test=True))\n",
        "\n",
        "test_results = eval_test(test_loader, model, edited_test_df['id'], \"bert\")\n",
        "\n",
        "test_results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYAwjlV73uCE"
      },
      "source": [
        "## 1.3 Fine-tuning DistilBERT\n",
        "\n",
        "The following code uses the pre-trained DistilBERT model as well as it's own embeddings in an attempt to solve the task. We fine tune the model, and evaluate it's performance on the test set as we have done with the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MNzcSa6ta_x"
      },
      "source": [
        "### Tokenize sentences and load into dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgxxIyeoId7-"
      },
      "source": [
        "def collate_fn_padd_BERT(batch, test=False):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "    if test:\n",
        "        batch_features = [cls + s1 + sep + s2 for (s1, s2) in batch]\n",
        "    else: \n",
        "        batch_features = [cls + s1 + sep + s2 for (s1, s2), l in batch]\n",
        "        batch_labels = [l for f, l in batch]\n",
        "\n",
        "    batch_features_len = [len(f) for f in batch_features]\n",
        "\n",
        "    seq_tensor = torch.zeros((len(batch_features), max(batch_features_len))).long()\n",
        "    mask_tensor = torch.zeros((len(batch_features), max(batch_features_len))).long()\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "        mask_tensor[idx, :seqlen] = torch.ones(seqlen)\n",
        "\n",
        "    # Put sequence and tensor together\n",
        "    feature_tensor = torch.stack([seq_tensor, mask_tensor])\n",
        "\n",
        "    if test:\n",
        "        return feature_tensor\n",
        "  \n",
        "    batch_labels = torch.LongTensor(batch_labels)\n",
        "    return feature_tensor, batch_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC4LgFjIxT2f"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "input_id_pairs = []\n",
        "\n",
        "cls = tokenizer.encode('[CLS]')\n",
        "sep = tokenizer.encode('[SEP]')\n",
        "\n",
        "for (sentence1, sentence2) in training_data:\n",
        "    encoded_sentence1 = tokenizer.encode(sentence1, add_special_tokens=False)\n",
        "    encoded_sentence2 = tokenizer.encode(sentence2, add_special_tokens=False)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_id_pairs.append([encoded_sentence1, encoded_sentence2])\n",
        "\n",
        "train_loader, dev_loader = create_dataloaders(input_id_pairs, labels, collate_fn_padd_BERT, flip=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_LfLzmrvsYW"
      },
      "source": [
        "### DistilBert for Humour detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtykWoYywy34"
      },
      "source": [
        "class HumourBert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HumourBert, self).__init__()\n",
        "        self.bert = DistilBertForSequenceClassification.from_pretrained(\n",
        "            'distilbert-base-uncased', \n",
        "            num_labels = 3,\n",
        "            output_attentions = False,\n",
        "            output_hidden_states = False\n",
        "        )\n",
        "    \n",
        "    def forward(self, feature):\n",
        "        b_input_ids = feature[0].to(device)\n",
        "        b_input_mask = feature[1].to(device)\n",
        "        outputs = self.bert(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_input_mask\n",
        "        )\n",
        "        return outputs.logits\n",
        "\n",
        "\n",
        "model = HumourBert().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Za81ssHv0zG"
      },
      "source": [
        "### Model Instantiation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEub9iOq7Hol"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = 1e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 16\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer, scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeBaH_4Kv7Zw"
      },
      "source": [
        "### Obtain test set predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmb2yf0udb4B"
      },
      "source": [
        "test_id_pairs = []\n",
        "\n",
        "for (sentence1, sentence2) in test_data:\n",
        "    encoded_sentence1 = tokenizer.encode(sentence1, add_special_tokens=False)\n",
        "    encoded_sentence2 = tokenizer.encode(sentence2, add_special_tokens=False)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    test_id_pairs.append([encoded_sentence1, encoded_sentence2])\n",
        "\n",
        "test_loader = create_test_dataloader(test_id_pairs, lambda b: collate_fn_BERT(b, test=True))\n",
        "\n",
        "test_results = eval_test(test_loader, model, edited_test_df['id'], \"bert2\")\n",
        "\n",
        "test_results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXBhmuk_m6zD"
      },
      "source": [
        "# Approach 2: No pre-trained representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b33Dsrx-2gho"
      },
      "source": [
        "## Create word-index mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdmmpE8G2rDW"
      },
      "source": [
        "vocab = Vocabulary()\r\n",
        "\r\n",
        "# 0-padding token\r\n",
        "vocab.add_word('<pad>')\r\n",
        "# sentence start\r\n",
        "vocab.add_word('<s>')\r\n",
        "# sentence end\r\n",
        "vocab.add_word('</s>')\r\n",
        "# Unknown words\r\n",
        "vocab.add_word('<unk>')\r\n",
        "vocab.build_from_list(joint_vocab)\r\n",
        "print(vocab)\r\n",
        "print(vocab.convert_words_to_idxs('the dancer is on the moon'.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFIn7ZCNDwUr"
      },
      "source": [
        "## Prepare data\n",
        "TODO: Reduce duplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-HqFrNGDzLF"
      },
      "source": [
        "vectorized_seqs = [[[vocab._word2idx[tok] for tok in sen if tok in vocab._word2idx] for sen in seq] for seq in training_tokenized_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcnsn3bfD7e_"
      },
      "source": [
        "labels = list(train_df['label'])\n",
        "\n",
        "train_loader, dev_loader = create_dataloaders(vectorized_seqs, labels, collate_fn_padd)\n",
        "\n",
        "print(\"Dataloaders created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVchsvzdCkNS"
      },
      "source": [
        "## Bi-LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX6tvWIPFJRl"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 16\n",
        "\n",
        "INPUT_DIM = len(vocab) + 1\n",
        "HIDDEN_DIM = 400 \n",
        "EMBEDDING_DIM = 300 \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "## TODO: Figure out network dimensions\n",
        "model = BiLSTM_classification(EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, BATCH_SIZE, device)\n",
        "print(\"Model initialised.\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW7vXWTpFOCB"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52LeFCPkej3c"
      },
      "source": [
        "## GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP39ihJKejAK"
      },
      "source": [
        "class GRU_classification(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, device, n_layers, drop_prob=0.2):\n",
        "        super(GRU_classification, self).__init__()\n",
        "        self.n_hidden = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.emb_drop = nn.Dropout(drop_prob)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=True)\n",
        "        self.hidden_to_out = nn.Linear(hidden_dim * 4, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, sentence):\n",
        "      self.hidden = self.init_hidden(sentence.shape[1])\n",
        "\n",
        "      embedding1 = self.emb_drop(self.embedding(sentence[0]))\n",
        "      embedding1 = embedding1.permute(1, 0, 2)\n",
        "      embedding2 = self.emb_drop(self.embedding(sentence[1]))\n",
        "      embedding2 = embedding2.permute(1, 0, 2)\n",
        "\n",
        "      GRU_out_1, _ = self.gru(embedding1, self.hidden)\n",
        "      GRU_out_2, _ = self.gru(embedding2, self.hidden)\n",
        "\n",
        "      out = self.hidden_to_out(self.relu(torch.cat([GRU_out_1[-1], GRU_out_2[-1]], 1)))\n",
        "      return out\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros((4, batch_size, self.n_hidden), requires_grad=True).to(self.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElVoeQp6hhiy"
      },
      "source": [
        "class BiLSTM_Attention_classification(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, device, num_layers=1, dropout_p=0.2, use_attn=True):\n",
        "        super(BiLSTM_Attention_classification, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device=device\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.emb_dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.use_attn = use_attn\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True, dropout=dropout_p)\n",
        "\n",
        "        if use_attn:\n",
        "            self.attn = nn.MultiheadAttention(hidden_dim * 2, num_heads=1)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedding1 = self.emb_dropout(self.embedding(sentence[0]))\n",
        "        embedding1 = embedding1.permute(1, 0, 2)\n",
        "\n",
        "        embedding2 = self.emb_dropout(self.embedding(sentence[1]))\n",
        "        embedding2 = embedding2.permute(1, 0, 2)\n",
        "\n",
        "        lstm_out_1, _ = self.lstm(embedding1.view(len(embedding1), -1, self.embedding_dim))\n",
        "        lstm_out_2, _ = self.lstm(embedding2.view(len(embedding2), -1, self.embedding_dim))\n",
        "\n",
        "        if self.use_attn:\n",
        "            out_1 = self.attn(lstm_out_1[-1].unsqueeze(0), lstm_out_1, lstm_out_1)[0].squeeze()\n",
        "            out_2 = self.attn(lstm_out_2[-1].unsqueeze(0), lstm_out_2, lstm_out_2)[0].squeeze()\n",
        "        else:\n",
        "            out_1, out_2 = lstm_out_1[-1], lstm_out_2[-1]\n",
        "        \n",
        "        # concat both lstm_out_1 and lstm_out_2 and give to our linear layer\n",
        "        # lstm_out_1[-1] are both batch_size x (2 * hidden_size)\n",
        "        # We concat so it's batch_size x (2*(2 * hidden_size))\n",
        "        out = self.hidden2label(torch.cat([out_1, out_2], 1))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo2GztW3gAAJ"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 16\n",
        "\n",
        "INPUT_DIM = len(vocab) + 1\n",
        "HIDDEN_DIM = 400 \n",
        "EMBEDDING_DIM = 300 \n",
        "BATCH_SIZE = 32\n",
        "OUTPUT_DIM = 3\n",
        "NUM_BI_LAYERS = 1\n",
        "NUM_LAYERS = 4\n",
        "\n",
        "## TODO: Figure out network dimensions\n",
        "model = BiLSTM_Attention_classification(EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, device, NUM_LAYERS, NUM_BI_LAYERS, use_attn=False)\n",
        "print(\"Model initialised.\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGbJCIjogdku"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAhN57qU2toN"
      },
      "source": [
        "## Pre-provided code for approach 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "V_l3AjbLm6zE"
      },
      "source": [
        "train_and_dev = train_df['edit1']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit1'], train_df['label'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "naive_model = MultinomialNB().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = naive_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = naive_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpKDFO8ym6zE"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhcP7AD6m6zE"
      },
      "source": [
        "# Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + 1  # 1 is most common class\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, torch.tensor(dev_y.values), True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}