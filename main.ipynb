{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willburr/humorous-headlines/blob/collate_fn/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEewKMPzm6ys"
      },
      "source": [
        "### Coursework coding instructions (please also see full coursework spec)\n",
        "\n",
        "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
        "\n",
        "For the task you choose you will need to do two approaches:\n",
        "  - Approach 1, which can use use pre-trained embeddings / models\n",
        "  - Approach 2, which should not use any pre-trained embeddings or models\n",
        "We should be able to run both approaches from the same colab file\n",
        "\n",
        "#### Running your code:\n",
        "  - Your models should run automatically when running your colab file without further intervention\n",
        "  - For each task you should automatically output the performance of both models\n",
        "  - Your code should automatically download any libraries required\n",
        "\n",
        "#### Structure of your code:\n",
        "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
        "  - Otherwise there are no restrictions on what you can do in your code\n",
        "\n",
        "#### Documentation:\n",
        "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
        "\n",
        "#### Reproducibility:\n",
        "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
        "\n",
        "Good luck! We are really looking forward to seeing your reports and your model code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd1-94s1MMgw"
      },
      "source": [
        "## Download and install Transformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTEI9ZFUwt0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4adce63-0d64-4541-cdce-f278ea039ea2"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=fb43cba5a1252ae9372b7c8d5db98143c7329d39dc208511dd0197d7bd4b58cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSraR3PdJ2i9"
      },
      "source": [
        "# Project  Set-up\r\n",
        "\r\n",
        "The code blocks below provide the imports and set-up steps to run later sections. We import necessary libraries, set pytorch to use the GPU and load and unzip the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFcxAGnSm6y2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c673e5-c617-416b-e8d8-f4ac39058a44"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import torch.optim as optim\n",
        "import codecs\n",
        "import tqdm\n",
        "\n",
        "# Helper code to perform stopword removal\n",
        "!pip install nltk \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Imports for use of BERT\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM2SBsBzm6y3"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVWAKu6ALaCL"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "Here we download the data files, load them into pandas and examine the first few columns of the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AEMK9jQjem-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d44ff66-2e7d-4032-ba79-44bd1b689c5c"
      },
      "source": [
        "!wget -O train.csv https://drive.google.com/u/0/uc?id=1KS6Cxl4CJnSLkMcdgbnmEdsStlVzacWX&export=download\n",
        "!wget -O dev.csv https://drive.google.com/u/0/uc?id=19WKir5IRn83NMcVgvNDrmgcDj1UIv7kt&export=download\n",
        "!wget -O test.csv https://drive.google.com/u/0/uc?id=11pyqg27tGRC1iDo26C2b01bMETqdUwuf&export=download"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-21 23:58:02--  https://drive.google.com/u/0/uc?id=1KS6Cxl4CJnSLkMcdgbnmEdsStlVzacWX\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.113, 74.125.142.101, 74.125.142.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/drc1onbhhm0l5hsrkqlpasokt18bdcni/1613951850000/13802342090854404605/*/1KS6Cxl4CJnSLkMcdgbnmEdsStlVzacWX [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-21 23:58:04--  https://doc-08-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/drc1onbhhm0l5hsrkqlpasokt18bdcni/1613951850000/13802342090854404605/*/1KS6Cxl4CJnSLkMcdgbnmEdsStlVzacWX\n",
            "Resolving doc-08-cc-docs.googleusercontent.com (doc-08-cc-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-08-cc-docs.googleusercontent.com (doc-08-cc-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1859231 (1.8M) [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   1.77M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2021-02-21 23:58:04 (215 MB/s) - ‘train.csv’ saved [1859231/1859231]\n",
            "\n",
            "--2021-02-21 23:58:04--  https://drive.google.com/u/0/uc?id=19WKir5IRn83NMcVgvNDrmgcDj1UIv7kt\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.100, 74.125.142.102, 74.125.142.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/456td1qg5i90rt85u6t4e3477rd9h8dd/1613951850000/13802342090854404605/*/19WKir5IRn83NMcVgvNDrmgcDj1UIv7kt [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-21 23:58:05--  https://doc-0c-cc-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/456td1qg5i90rt85u6t4e3477rd9h8dd/1613951850000/13802342090854404605/*/19WKir5IRn83NMcVgvNDrmgcDj1UIv7kt\n",
            "Resolving doc-0c-cc-docs.googleusercontent.com (doc-0c-cc-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0c-cc-docs.googleusercontent.com (doc-0c-cc-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 424235 (414K) [text/csv]\n",
            "Saving to: ‘dev.csv’\n",
            "\n",
            "dev.csv             100%[===================>] 414.29K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-02-21 23:58:05 (148 MB/s) - ‘dev.csv’ saved [424235/424235]\n",
            "\n",
            "--2021-02-21 23:58:05--  https://drive.google.com/u/0/uc?id=11pyqg27tGRC1iDo26C2b01bMETqdUwuf\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.135.102, 74.125.135.113, 74.125.135.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.135.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-70-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d42poe7i6rkif1pgn1vrmusfckfpomm3/1613951850000/18042724966187936417/*/11pyqg27tGRC1iDo26C2b01bMETqdUwuf [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-21 23:58:06--  https://doc-04-70-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d42poe7i6rkif1pgn1vrmusfckfpomm3/1613951850000/18042724966187936417/*/11pyqg27tGRC1iDo26C2b01bMETqdUwuf\n",
            "Resolving doc-04-70-docs.googleusercontent.com (doc-04-70-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-04-70-docs.googleusercontent.com (doc-04-70-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 525642 (513K) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 513.32K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-02-21 23:58:06 (109 MB/s) - ‘test.csv’ saved [525642/525642]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VU17WM0m6y3"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TevikPkl010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9114602b-516e-46ea-ebd1-c689c49d2e4d"
      },
      "source": [
        "# Take a look at the data\r\n",
        "test_df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original1</th>\n",
              "      <th>edit1</th>\n",
              "      <th>original2</th>\n",
              "      <th>edit2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>704-2704</td>\n",
              "      <td>\" Pence Is Trying to Control Republican Politi...</td>\n",
              "      <td>barbers</td>\n",
              "      <td>\" Pence Is Trying to &lt;Control/&gt; Republican Pol...</td>\n",
              "      <td>Bungle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>704-14395</td>\n",
              "      <td>\" Pence Is Trying to Control Republican Politi...</td>\n",
              "      <td>barbers</td>\n",
              "      <td>\" &lt;Pence/&gt; Is Trying to Control Republican Pol...</td>\n",
              "      <td>Witch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2704-14395</td>\n",
              "      <td>\" Pence Is Trying to &lt;Control/&gt; Republican Pol...</td>\n",
              "      <td>Bungle</td>\n",
              "      <td>\" &lt;Pence/&gt; Is Trying to Control Republican Pol...</td>\n",
              "      <td>Witch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11098-10186</td>\n",
              "      <td>\" There is no Man Behind the Curtain , \" says ...</td>\n",
              "      <td>elephant</td>\n",
              "      <td>\" There is no Man Behind the Curtain , \" says ...</td>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11098-2118</td>\n",
              "      <td>\" There is no Man Behind the Curtain , \" says ...</td>\n",
              "      <td>elephant</td>\n",
              "      <td>\" There is no Man Behind the Curtain , \" says ...</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ...   edit2\n",
              "0     704-2704  ...  Bungle\n",
              "1    704-14395  ...   Witch\n",
              "2   2704-14395  ...   Witch\n",
              "3  11098-10186  ...   woman\n",
              "4   11098-2118  ...     man\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSRS6PLHLAtf"
      },
      "source": [
        "## Download and unzip GLOVE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydxDrWAEpSjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88272ad6-61b9-4042-dac7-fd4acd1723bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlEA5Hhym6y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d3c6ce-133f-4417-db11-16c82ea3c047"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-21 23:58:14--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-21 23:58:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-21 23:58:14--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip         50%[=========>          ] 416.00M  2.00MB/s    eta 3m 24s ^C\n",
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riXUwYsk9-TO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ac2156-ba88-4640-d324-925ffe299bac"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/cw/glove.6B.zip'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/cw/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ErHEDpRMfSv"
      },
      "source": [
        "## Training Evaluation Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZKyXkuFm6y5"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch, optimizer, scheduler=None):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Training model.\")\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "        \n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_correct = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "            feature, target = batch\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            correct, __ = model_performance(np.argmax(predictions.detach().cpu().numpy(), axis=1), target.detach().cpu().numpy())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if scheduler:\n",
        "              scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_correct += correct\n",
        "\n",
        "        valid_loss, valid_acc, __, __ = eval(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_acc = epoch_loss / no_observations, epoch_correct / no_observations\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train Accuracy: {epoch_acc:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. Accuracy: {valid_acc:.2f} |')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUmf4mSTm6y6"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "            \n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            correct, __ = model_performance(np.argmax(pred, axis=1), trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_correct += correct\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_correct/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wre27ok7m6y7"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    correct_answers = (output == target)\n",
        "    correct = sum(correct_answers)\n",
        "    acc = np.true_divide(correct,len(output))\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| Acc: {acc:.2f} ')\n",
        "\n",
        "    return correct, acc"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWsWCL8NNBR0"
      },
      "source": [
        "def eval_test(data_iter, model, ids, filename):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    pred_all = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            \n",
        "            batch = batch.to(device)\n",
        "            # for RNN:\n",
        "            model.batch_size = batch.shape[1]\n",
        "\n",
        "            predictions = model(batch).squeeze(1)\n",
        "\n",
        "            pred = predictions.detach().cpu().numpy()\n",
        "\n",
        "            pred_all.extend(pred)\n",
        "\n",
        "    pred_all = np.argmax(np.array(pred_all), axis=1)\n",
        "\n",
        "    combined = list(zip(ids, pred_all))\n",
        "\n",
        "    res = pd.DataFrame(combined, columns=['id', 'pred'])\n",
        "    res.set_index('id', inplace=True, drop=True)\n",
        "\n",
        "    res.to_csv(filename + '.csv')\n",
        "\n",
        "    return res"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L9ZReWeMo7M"
      },
      "source": [
        "# Data augmentation and Vocab Preparation\n",
        "\n",
        "The following section provides the functions used to augment the data such that we flip the ordering of input sentences and flip the resulting label. We also define the classes and functions used for creating the vocabulary data structure that maintains mappings between words and indexes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2qnkCm2M5QK"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "The substitute function performs the injection of the edit word into the original sentence. This is then used in get_edited_df to return the dataframe with columns containing the edited sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkq5h2NF-VnS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bfefd163-d516-413a-e46b-685fe57d207a"
      },
      "source": [
        "import re\n",
        "\n",
        "def substitute(sentence, edit):\n",
        "  open_pos = sentence.find('<')\n",
        "  close_pos = sentence.find('>')\n",
        "  sub = sentence.replace(sentence[open_pos: close_pos + 1], edit)\n",
        "  return sub\n",
        "\n",
        "def get_edited_df(df, test=False):\n",
        "  id = df['id']\n",
        "  edited1 = df.apply(lambda x:substitute(x['original1'], x['edit1']), axis=1)\n",
        "  edited2 = df.apply(lambda x:substitute(x['original2'], x['edit2']), axis=1)\n",
        "  combined = list(zip(edited1,edited2))\n",
        "  if test:\n",
        "    id = df['id']\n",
        "    combined = list(zip(id,edited1,edited2))\n",
        "    return pd.DataFrame(combined, columns=['id','edited1', 'edited2'])\n",
        "  return pd.DataFrame(combined, columns=['edited1', 'edited2'])\n",
        "\n",
        "edited_train_df = get_edited_df(train_df)\n",
        "edited_test_df = get_edited_df(test_df, True)\n",
        "\n",
        "edited_train_df.head()\n",
        "# edited_test_df.head()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>edited1</th>\n",
              "      <th>edited2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\" Gene Cernan , Last Dancer on the Moon , Dies...</td>\n",
              "      <td>\" Gene Cernan , Last Astronaut on the Moon , i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\" I 'm done \" : Fed up with California , some ...</td>\n",
              "      <td>\" I 'm done \" : Fed up with pancakes , some co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\" I 'm done \" : Fed up with California , some ...</td>\n",
              "      <td>\" I 'm done \" : Fed up with life , some conser...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\" I 'm done \" : Fed up with pancakes , some co...</td>\n",
              "      <td>\" I 'm done \" : Fed up with life , some conser...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\" Our expectations of what civic engagement lo...</td>\n",
              "      <td>\" Our expectations of what civic engagement sm...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             edited1                                            edited2\n",
              "0  \" Gene Cernan , Last Dancer on the Moon , Dies...  \" Gene Cernan , Last Astronaut on the Moon , i...\n",
              "1  \" I 'm done \" : Fed up with California , some ...  \" I 'm done \" : Fed up with pancakes , some co...\n",
              "2  \" I 'm done \" : Fed up with California , some ...  \" I 'm done \" : Fed up with life , some conser...\n",
              "3  \" I 'm done \" : Fed up with pancakes , some co...  \" I 'm done \" : Fed up with life , some conser...\n",
              "4  \" Our expectations of what civic engagement lo...  \" Our expectations of what civic engagement sm..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp28SsNZ5HQR"
      },
      "source": [
        "## Define Vocab Class\r\n",
        "\r\n",
        "This class stores mappings from word to index and index to words. It also provides utility functions to build the mappings from files or a list data structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3q9gLjl5TOq"
      },
      "source": [
        "class Vocabulary(object):\r\n",
        "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\r\n",
        "  def __init__(self):\r\n",
        "    # Mapping from tokens to integers\r\n",
        "    self._word2idx = {}\r\n",
        "\r\n",
        "    # Reverse-mapping from integers to tokens\r\n",
        "    self.idx2word = []\r\n",
        "\r\n",
        "  def word2idx(self, word, default=None):\r\n",
        "    \"\"\"Returns the integer ID of the word or default if not found.\"\"\"\r\n",
        "    return self._word2idx.get(word, default)\r\n",
        "\r\n",
        "  def add_word(self, word):\r\n",
        "    \"\"\"Adds the `word` into the vocabulary.\"\"\"\r\n",
        "    if word not in self._word2idx:\r\n",
        "      self.idx2word.append(word)\r\n",
        "      self._word2idx[word] = len(self.idx2word) - 1\r\n",
        "\r\n",
        "  def build_from_list(self, words):\r\n",
        "    for word in words:\r\n",
        "      self.add_word(word)\r\n",
        "\r\n",
        "  def build_from_file(self, fname):\r\n",
        "    \"\"\"Builds a vocabulary from a given corpus file.\"\"\"\r\n",
        "    with open(fname) as f:\r\n",
        "      for line in f:\r\n",
        "        words = line.strip().split()\r\n",
        "        for word in words:\r\n",
        "          self.add_word(word)\r\n",
        "\r\n",
        "  def convert_idxs_to_words(self, idxs):\r\n",
        "    \"\"\"Converts a list of indices to words.\"\"\"\r\n",
        "    return ' '.join(self.idx2word[idx] for idx in idxs)\r\n",
        "\r\n",
        "  def convert_words_to_idxs(self, words):\r\n",
        "    \"\"\"Converts a list of words to a list of indices.\"\"\"\r\n",
        "    return [self.word2idx(w) for w in words]\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    \"\"\"Returns the size of the vocabulary.\"\"\"\r\n",
        "    return len(self.idx2word)\r\n",
        "  \r\n",
        "  def __repr__(self):\r\n",
        "    return \"Vocabulary with {} items\".format(self.__len__())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbw3d6ZFM8_E"
      },
      "source": [
        "## Vocab Preparation\n",
        "\n",
        "create_vocab allows for the tokenization of the corpus and creation of a list of words to be passed to the vocabulary. It provides two flags which the user can set to execute data cleaning operations of punctuation and stopword removal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zub4c6BJm6y8"
      },
      "source": [
        "def create_vocab(data, remove_punc=True, remove_stopwords=True):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    for sentence_pair in data:\n",
        "        tokenized_sentence_pair = []\n",
        "        for sentence in sentence_pair:\n",
        "            tokenized_sentence = []\n",
        "\n",
        "            # Split on spaces\n",
        "            tokens = sentence.split(' ')\n",
        "            \n",
        "            # Remove punctuation\n",
        "            if remove_punc:\n",
        "              tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "            # Remove stopwords\n",
        "            if remove_stopwords:\n",
        "              tokens = [word for word in tokens if not word in stop_words]\n",
        "\n",
        "            for token in tokens: \n",
        "\n",
        "                tokenized_sentence.append(token.lower())\n",
        "\n",
        "            tokenized_sentence_pair.append(tokenized_sentence)\n",
        "        tokenized_corpus.append(tokenized_sentence_pair)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence_pair in tokenized_corpus:\n",
        "\n",
        "        for sentence in sentence_pair:\n",
        "\n",
        "            for token in sentence:\n",
        "\n",
        "                if token.lower() not in vocabulary:\n",
        "\n",
        "                    vocabulary.append(token.lower())\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTQ_arcRNEjK"
      },
      "source": [
        "## Load the Dataset\n",
        "\n",
        "In this section, we present the code that is used to create train and validation pytorch dataloaders with the data augmentation of flipping the input sentences applied to the train dataloader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHerrK5LNJbG"
      },
      "source": [
        "### Utility functions\n",
        "\n",
        "We create a class to hold the dataset to be loaded into the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCEObxVMKUad"
      },
      "source": [
        "# We create a Dataset so we can create minibatches\n",
        "class Task2Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, train_data, labels):\n",
        "        self.x_train = train_data\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train[item], self.y_train[item]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD5PF9bnYsRk"
      },
      "source": [
        "### Creating Dataloaders\n",
        "\n",
        "The first function, flip_label, is used in the data augmentation process. Then the two following functions are used to create and return pytorch dataloaders for the training and test datasets. note that create_dataloaders splits the dataset into a training and validation set dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yXQMGWHW_ti"
      },
      "source": [
        "def flip_label(label):\n",
        "  if label == 1:\n",
        "    return 2\n",
        "  elif label == 2:\n",
        "    return 1\n",
        "  else: \n",
        "    return label\n",
        "\n",
        "def create_dataloaders(feature, labels, collate_fn, train_proportion=0.8, batch_size=32, flip=True):\n",
        "    # 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
        "    train_and_dev = Task2Dataset(feature, labels)\n",
        "\n",
        "    train_examples = round(len(train_and_dev)*train_proportion)\n",
        "    dev_examples = len(train_and_dev) - train_examples\n",
        "\n",
        "    train_dataset, dev_dataset = random_split(train_and_dev,\n",
        "                                              (train_examples,\n",
        "                                                dev_examples))\n",
        "\n",
        "    if flip:\n",
        "        # Add the reverse examples to the training set to create more training data\n",
        "        flipped_train_dataset = []\n",
        "        for r in train_dataset:\n",
        "          flipped_train_dataset.append((list(reversed(r[0])), flip_label(r[1])))\n",
        "        train_dataset += flipped_train_dataset\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    \n",
        "    return train_loader, dev_loader\n",
        "\n",
        "def create_test_dataloader(feature, collate_fn, batch_size=32):\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(feature, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    \n",
        "    return test_loader\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvXJ20y4m6y4"
      },
      "source": [
        "# Approach 1: Using pre-trained representations\n",
        "\n",
        "In this section, we explore three different methods for learning which of the two edited sentences are funnier. We first explore the use of the GloVe embeddings with a BiLSTM, then we use BERT embeddings with the same model structure and finally, we use the entire BERT architecture, including the transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SiCAvnONQCO"
      },
      "source": [
        "## 1.1 Using pre-trained (GloVe) embeddings with a BiLSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FlQVqkEICEE"
      },
      "source": [
        "###Create vocab and load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szdwn4PS-9_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5182c253-b466-4cb7-8822-f62b009e10e0"
      },
      "source": [
        "# We set our training data and test data\n",
        "training_data = edited_train_df.values.tolist()\n",
        "test_data = np.array(edited_test_df.values.tolist())[:,1:].tolist()\n",
        "\n",
        "# Creating word vectors\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
        "\n",
        "# Creating joint vocab from test and train\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data + test_data)\n",
        "\n",
        "print(\"Vocab created.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGQJ_QNYFu_p"
      },
      "source": [
        "# We create representations for our tokens\n",
        "wvecs = [] # word vectors\n",
        "\n",
        "vocab = Vocabulary()\n",
        "# This is a large file, it will take a while to load in the memory!\n",
        "with codecs.open('glove.6B.300d.txt', 'r','utf-8') as f:\n",
        "  index = 1\n",
        "  for line in f.readlines():\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in joint_vocab:\n",
        "          (word, vec) = (word,\n",
        "                     list(map(float,line.strip().split()[1:])))\n",
        "          wvecs.append(vec)\n",
        "          vocab.add_word(word)\n",
        "\n",
        "\n",
        "wvecs = np.array(wvecs)\n",
        "\n",
        "vectorized_seqs = [[[vocab.word2idx(tok) for tok in sen if tok in vocab._word2idx] for sen in seq] for seq in training_tokenized_corpus]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Q_rgNjIN0L"
      },
      "source": [
        "# Used for collating our observations into minibatches:\n",
        "def collate_fn_padd(batch, test=False):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "    if test:\n",
        "        batch_features = batch\n",
        "    else:\n",
        "        batch_labels = [l for f, l in batch]\n",
        "        batch_features = [f for f, l in batch]\n",
        "    \n",
        "    batch_features_len = [[len(s) for s in b] for b in batch_features]\n",
        "\n",
        "    seq_tensor = torch.zeros((2, len(batch_features), np.max(batch_features_len))).long()\n",
        "\n",
        "    # Shape of seq_tensor is batch_size x max_feature_len\n",
        "    # It should be batch_size x 2 x max_feature_len\n",
        "\n",
        "    for idx, (seq, seqlens) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        for i in range(2):\n",
        "            seq_tensor[i, idx, :seqlens[i]] = torch.LongTensor(seq[i])\n",
        "\n",
        "    if test:\n",
        "      return seq_tensor\n",
        "\n",
        "    batch_labels = torch.LongTensor(batch_labels)\n",
        "    return seq_tensor, batch_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKwiK_pKQbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3800081a-4602-4fe9-9206-472eca535bf1"
      },
      "source": [
        "feature = vectorized_seqs\n",
        "labels = list(train_df['label'])\n",
        "\n",
        "train_loader, dev_loader = create_dataloaders(feature, labels, collate_fn_padd)\n",
        "\n",
        "print(\"Dataloaders created.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataloaders created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eraY3strIOrJ"
      },
      "source": [
        "### BiLSTM Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYyl1tpwm6y-"
      },
      "source": [
        "class BiLSTM_classification(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, device, num_layers=1, dropout_p=0.2):\n",
        "        super(BiLSTM_classification, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device=device\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.emb_dropout = nn.Dropout(dropout_p)\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True, dropout=dropout_p)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 4, 3)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedding1 = self.emb_dropout(self.embedding(sentence[0]))\n",
        "        embedding1 = embedding1.permute(1, 0, 2)\n",
        "\n",
        "        embedding2 = self.emb_dropout(self.embedding(sentence[1]))\n",
        "        embedding2 = embedding2.permute(1, 0, 2)\n",
        "\n",
        "        lstm_out_1, _ = self.lstm(embedding1.view(len(embedding1), -1, self.embedding_dim))\n",
        "        lstm_out_2, _ = self.lstm(embedding2.view(len(embedding2), -1, self.embedding_dim))\n",
        "        \n",
        "        # concat both lstm_out_1 and lstm_out_2 and give to our linear layer\n",
        "        # lstm_out_1[-1] are both batch_size x (2 * hidden_size)\n",
        "        # We concat so it's batch_size x (2*(2 * hidden_size))\n",
        "        out = self.hidden2label(torch.cat([lstm_out_1[-1], lstm_out_2[-1]], 1))\n",
        "        return out"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD_1RRbvIVPC"
      },
      "source": [
        "### Model Instantiation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJtG2gnyD5AU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "6cbfe1c7-acf8-450c-c589-7415585dc814"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 20\n",
        "\n",
        "INPUT_DIM = len(vocab) + 1\n",
        "HIDDEN_DIM = 200 \n",
        "EMBEDDING_DIM = 300\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "model = BiLSTM_classification(EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, device, num_layers, dropout)\n",
        "print(\"Model initialised.\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# We provide the model with our embeddings\n",
        "model.embedding.weight.data[1:].copy_(torch.from_numpy(wvecs))\n",
        "# Freeze model embeddings\n",
        "model.embedding.weight.requires_grad = False\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.96 | Train Accuracy: 0.46 |         Val. Loss: 0.97 | Val. Accuracy: 0.45 |\n",
            "| Epoch: 02 | Train Loss: 0.94 | Train Accuracy: 0.51 |         Val. Loss: 0.95 | Val. Accuracy: 0.52 |\n",
            "| Epoch: 03 | Train Loss: 0.91 | Train Accuracy: 0.55 |         Val. Loss: 0.94 | Val. Accuracy: 0.52 |\n",
            "| Epoch: 04 | Train Loss: 0.83 | Train Accuracy: 0.62 |         Val. Loss: 0.92 | Val. Accuracy: 0.56 |\n",
            "| Epoch: 05 | Train Loss: 0.72 | Train Accuracy: 0.69 |         Val. Loss: 0.96 | Val. Accuracy: 0.58 |\n",
            "| Epoch: 06 | Train Loss: 0.59 | Train Accuracy: 0.76 |         Val. Loss: 1.01 | Val. Accuracy: 0.56 |\n",
            "| Epoch: 07 | Train Loss: 0.51 | Train Accuracy: 0.79 |         Val. Loss: 1.09 | Val. Accuracy: 0.57 |\n",
            "| Epoch: 08 | Train Loss: 0.42 | Train Accuracy: 0.83 |         Val. Loss: 1.26 | Val. Accuracy: 0.53 |\n",
            "| Epoch: 09 | Train Loss: 0.36 | Train Accuracy: 0.86 |         Val. Loss: 1.20 | Val. Accuracy: 0.55 |\n",
            "| Epoch: 10 | Train Loss: 0.31 | Train Accuracy: 0.88 |         Val. Loss: 1.37 | Val. Accuracy: 0.59 |\n",
            "| Epoch: 11 | Train Loss: 0.28 | Train Accuracy: 0.89 |         Val. Loss: 1.32 | Val. Accuracy: 0.58 |\n",
            "| Epoch: 12 | Train Loss: 0.25 | Train Accuracy: 0.90 |         Val. Loss: 1.43 | Val. Accuracy: 0.61 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-69181af0605f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-6650ebce0001>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, dev_iter, model, number_epoch, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcL6evN-OkhI"
      },
      "source": [
        "### Obtain test set predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZksGlYFGCJaL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c9d8675c-523f-4475-a484-aa71d3e6b0cb"
      },
      "source": [
        "test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
        "vectorized_test = [[[vocab.word2idx(tok) for tok in sen if tok in vocab._word2idx] for sen in seq] for seq in test_tokenized_corpus]\n",
        "\n",
        "test_features = vectorized_test\n",
        "test_loader = create_test_dataloader(test_features, lambda b: collate_fn_padd(b, test=True))\n",
        "\n",
        "test_results = eval_test(test_loader, model, edited_test_df['id'], \"lstm\")\n",
        "\n",
        "test_results.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>704-2704</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704-14395</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2704-14395</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11098-10186</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11098-2118</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             pred\n",
              "id               \n",
              "704-2704        1\n",
              "704-14395       1\n",
              "2704-14395      1\n",
              "11098-10186     1\n",
              "11098-2118      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX1hPU1F1liM"
      },
      "source": [
        "## 1.2 Using DistilBERT embeddings\n",
        "\n",
        "In this section, we will tokenize the sentences using DistilBERT's tokenizer and load the embeddings into a BiLSTM model of the same structure used in the previous section.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQPL8jfTquMW"
      },
      "source": [
        "### Tokenize sentences and load into dataloaders "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FupE9L_zIWWc"
      },
      "source": [
        "# Note that the following ones add an attention mask (used to tell BERT to ignore padding)\n",
        "def collate_fn_padd_BERT_embeddings(batch, test=False):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "    if test:\n",
        "        batch_features = batch\n",
        "    else: \n",
        "        batch_labels = [l for f, l in batch]\n",
        "        batch_features = [f for f, l in batch]\n",
        "\n",
        "    batch_features_len = [[len(s) for s in b] for b in batch_features]\n",
        "\n",
        "    seq_tensor = torch.zeros((2, len(batch), np.max(batch_features_len))).long()\n",
        "    mask_tensor = torch.zeros((2, len(batch), np.max(batch_features_len))).long()\n",
        "\n",
        "    # Shape of seq_tensor is batch_size x max_feature_len\n",
        "    # It should be batch_size x 2 x max_feature_len\n",
        "\n",
        "    for idx, (seq, seqlens) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        for i in range(2):\n",
        "            seq_tensor[i, idx, :seqlens[i]] = torch.LongTensor(seq[i])\n",
        "            mask_tensor[i, idx, :seqlens[i]] = torch.ones(seqlens[i])\n",
        "\n",
        "    feature_tensor = torch.stack([seq_tensor, mask_tensor], dim=1)\n",
        "\n",
        "    if test:\n",
        "        return feature_tensor\n",
        "\n",
        "    batch_labels = torch.LongTensor(batch_labels)\n",
        "    return feature_tensor, batch_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTIMdGWn4-AB"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "input_id_pairs = []\n",
        "\n",
        "for (sentence1, sentence2) in training_data:\n",
        "    encoded_sentence1 = tokenizer.encode(sentence1)\n",
        "    encoded_sentence2 = tokenizer.encode(sentence2)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_id_pairs.append([encoded_sentence1, encoded_sentence2])\n",
        "\n",
        "train_loader, dev_loader = create_dataloaders(input_id_pairs, labels, collate_fn_padd_BERT_embeddings)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b4T4VhqtS5"
      },
      "source": [
        "### BiLSTM model with DistilBERT embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6O_qyV_QpX"
      },
      "source": [
        "class BiLSTM_BERT_classification(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim, num_layers=1):\n",
        "        super(BiLSTM_BERT_classification, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased',\n",
        "            output_hidden_states = True, \n",
        "        )\n",
        "        # The word embedding dimension given by concatenating last 4 hidden \n",
        "        # layers of BERT\n",
        "        self.embedding_dim = 3072\n",
        "\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim, num_layers, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 4, 3)\n",
        "\n",
        "    def compute_embeddings(self, feature):\n",
        "        b_input_ids = feature[0].to(device)\n",
        "        b_input_mask = feature[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # We aren't trying to train BERT here, only use its embeddings\n",
        "            self.bert.eval()\n",
        "            outputs = self.bert(\n",
        "                b_input_ids,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "            hidden_states = outputs.hidden_states\n",
        "        \n",
        "        return torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        embedding1 = self.compute_embeddings(sentences[0])\n",
        "        embedding1 = embedding1.permute(1, 0, 2)\n",
        "        embedding2 = self.compute_embeddings(sentences[1])\n",
        "        embedding2 = embedding2.permute(1, 0, 2)\n",
        "\n",
        "        lstm_out_1, _ = self.lstm(embedding1.view(len(embedding1), -1, self.embedding_dim))\n",
        "        lstm_out_2, _ = self.lstm(embedding2.view(len(embedding2), -1, self.embedding_dim))\n",
        "        # concat both lstm_out_1 and lstm_out_2 and give to our linear layer, think we need\n",
        "        # output shape of lstm_ out is 2 * hidden_size as we do it in both directions\n",
        " \n",
        "        # lstm_out_1[-1] are both batch_size x (2 * hidden_size)\n",
        "        # do we concat so it's (2*batch_size) x (2 * hidden_size) or batch_size x (2*(2 * hidden_size))? - I chose the latter\n",
        "        out = self.hidden2label(torch.cat((lstm_out_1[-1], lstm_out_2[-1]), 1))\n",
        "        return out"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2P3zLeqrCLR"
      },
      "source": [
        "### Model Instantiation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnHthd2vRugs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "9c37e26d-5842-4a1f-be5b-6fb134004e5e"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 8\n",
        "\n",
        "HIDDEN_DIM = 400 # Higher appears to yield better val acc\n",
        "num_layers = 4\n",
        "\n",
        "## TODO: Figure out network dimensions\n",
        "model = BiLSTM_BERT_classification(HIDDEN_DIM, num_layers)\n",
        "print(\"Model initialised.\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-b14fb76c619b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-6650ebce0001>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, dev_iter, model, number_epoch, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sul58j7jrUKt"
      },
      "source": [
        "### Obtain test set predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2oL9RyRNRV-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "585783bb-94b9-4c74-cb89-f0e6f26d75e9"
      },
      "source": [
        "test_id_pairs = []\n",
        "\n",
        "for (sentence1, sentence2) in test_data:\n",
        "    encoded_sentence1 = tokenizer.encode(sentence1)\n",
        "    encoded_sentence2 = tokenizer.encode(sentence2)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    test_id_pairs.append([encoded_sentence1, encoded_sentence2])\n",
        "\n",
        "test_loader = create_test_dataloader(test_id_pairs, lambda b: collate_fn_padd_BERT_embeddings(b, test=True))\n",
        "\n",
        "test_results = eval_test(test_loader, model, edited_test_df['id'], \"bert\")\n",
        "\n",
        "test_results.head()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>704-2704</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704-14395</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2704-14395</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11098-10186</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11098-2118</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             pred\n",
              "id               \n",
              "704-2704        2\n",
              "704-14395       2\n",
              "2704-14395      2\n",
              "11098-10186     2\n",
              "11098-2118      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYAwjlV73uCE"
      },
      "source": [
        "## 1.3 Fine-tuning DistilBERT\n",
        "\n",
        "The following code uses the pre-trained DistilBERT model as well as it's own embeddings in an attempt to solve the task. We fine tune the model, and evaluate it's performance on the test set as we have done with the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MNzcSa6ta_x"
      },
      "source": [
        "### Tokenize sentences and load into dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgxxIyeoId7-"
      },
      "source": [
        "def collate_fn_padd_BERT(batch, test=False):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "    if test:\n",
        "        batch_features = [cls + s1 + sep + s2 for (s1, s2) in batch]\n",
        "    else: \n",
        "        batch_features = [cls + s1 + sep + s2 for (s1, s2), l in batch]\n",
        "        batch_labels = [l for f, l in batch]\n",
        "\n",
        "    batch_features_len = [len(f) for f in batch_features]\n",
        "\n",
        "    seq_tensor = torch.zeros((len(batch_features), max(batch_features_len))).long()\n",
        "    mask_tensor = torch.zeros((len(batch_features), max(batch_features_len))).long()\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "        mask_tensor[idx, :seqlen] = torch.ones(seqlen)\n",
        "\n",
        "    # Put sequence and tensor together\n",
        "    feature_tensor = torch.stack([seq_tensor, mask_tensor])\n",
        "\n",
        "    if test:\n",
        "        return feature_tensor\n",
        "  \n",
        "    batch_labels = torch.LongTensor(batch_labels)\n",
        "    return feature_tensor, batch_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC4LgFjIxT2f"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "input_id_pairs = []\n",
        "\n",
        "cls = tokenizer.encode('[CLS]')\n",
        "sep = tokenizer.encode('[SEP]')\n",
        "\n",
        "for (sentence1, sentence2) in training_data:\n",
        "    encoded_sentence1 = tokenizer.encode(sentence1, add_special_tokens=False)\n",
        "    encoded_sentence2 = tokenizer.encode(sentence2, add_special_tokens=False)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_id_pairs.append([encoded_sentence1, encoded_sentence2])\n",
        "\n",
        "train_loader, dev_loader = create_dataloaders(input_id_pairs, labels, collate_fn_padd_BERT, flip=False)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_LfLzmrvsYW"
      },
      "source": [
        "### DistilBert for Humour detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtykWoYywy34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926aba89-4067-4422-974d-190acee2607c"
      },
      "source": [
        "class HumourBert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HumourBert, self).__init__()\n",
        "        self.bert = DistilBertForSequenceClassification.from_pretrained(\n",
        "            'distilbert-base-uncased', \n",
        "            num_labels = 3,\n",
        "            output_attentions = False,\n",
        "            output_hidden_states = False\n",
        "        )\n",
        "    \n",
        "    def forward(self, feature):\n",
        "        b_input_ids = feature[0].to(device)\n",
        "        b_input_mask = feature[1].to(device)\n",
        "        outputs = self.bert(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_input_mask\n",
        "        )\n",
        "        return outputs.logits\n",
        "\n",
        "\n",
        "model = HumourBert().to(device)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Za81ssHv0zG"
      },
      "source": [
        "### Model Instantiation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEub9iOq7Hol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "28edaa82-ea4a-48ba-d167-bbc3a51ba4dd"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = 1e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 16\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer, scheduler)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.97 | Train Accuracy: 0.44 |         Val. Loss: 0.97 | Val. Accuracy: 0.44 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-6cf6841411fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-6650ebce0001>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, dev_iter, model, number_epoch, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeBaH_4Kv7Zw"
      },
      "source": [
        "### Obtain test set predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmb2yf0udb4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b62aaa27-183a-42f5-a562-c1a4988b5a56"
      },
      "source": [
        "test_id_pairs = []\n",
        "\n",
        "for (sentence1, sentence2) in test_data:\n",
        "    encoded_sentence1 = tokenizer.encode(sentence1, add_special_tokens=False)\n",
        "    encoded_sentence2 = tokenizer.encode(sentence2, add_special_tokens=False)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    test_id_pairs.append([encoded_sentence1, encoded_sentence2])\n",
        "\n",
        "test_loader = create_test_dataloader(test_id_pairs, lambda b: collate_fn_BERT(b, test=True))\n",
        "\n",
        "test_results = eval_test(test_loader, model, edited_test_df['id'], \"bert2\")\n",
        "\n",
        "test_results.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>704-2704</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704-14395</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2704-14395</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11098-10186</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11098-2118</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             pred\n",
              "id               \n",
              "704-2704        2\n",
              "704-14395       2\n",
              "2704-14395      2\n",
              "11098-10186     2\n",
              "11098-2118      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXBhmuk_m6zD"
      },
      "source": [
        "# Approach 2: No pre-trained representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b33Dsrx-2gho"
      },
      "source": [
        "## Create word-index mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdmmpE8G2rDW"
      },
      "source": [
        "vocab = Vocabulary()\r\n",
        "\r\n",
        "# 0-padding token\r\n",
        "vocab.add_word('<pad>')\r\n",
        "# sentence start\r\n",
        "vocab.add_word('<s>')\r\n",
        "# sentence end\r\n",
        "vocab.add_word('</s>')\r\n",
        "# Unknown words\r\n",
        "vocab.add_word('<unk>')\r\n",
        "vocab.build_from_list(joint_vocab)\r\n",
        "print(vocab)\r\n",
        "print(vocab.convert_words_to_idxs('the dancer is on the moon'.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFIn7ZCNDwUr"
      },
      "source": [
        "## Prepare data\n",
        "TODO: Reduce duplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-HqFrNGDzLF"
      },
      "source": [
        "vectorized_seqs = [[[vocab._word2idx[tok] for tok in sen if tok in vocab._word2idx] for sen in seq] for seq in training_tokenized_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcnsn3bfD7e_"
      },
      "source": [
        "train_loader, dev_loader = create_dataloaders(input_id_pairs, training_labels, collate_fn_padd)\n",
        "\n",
        "print(\"Dataloaders created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVchsvzdCkNS"
      },
      "source": [
        "## Bi-LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX6tvWIPFJRl"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 16\n",
        "\n",
        "INPUT_DIM = len(vocab) + 1\n",
        "HIDDEN_DIM = 400 \n",
        "EMBEDDING_DIM = 300 \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "## TODO: Figure out network dimensions\n",
        "model = BiLSTM_classification(EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, BATCH_SIZE, device)\n",
        "print(\"Model initialised.\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW7vXWTpFOCB"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52LeFCPkej3c"
      },
      "source": [
        "## GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP39ihJKejAK"
      },
      "source": [
        "class GRU_classification(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, device, n_layers, drop_prob=0.2):\n",
        "        super(GRU_classification, self).__init__()\n",
        "        self.n_hidden = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.emb_drop = nn.Dropout(drop_prob)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=True)\n",
        "        self.hidden_to_out = nn.Linear(hidden_dim * 4, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, sentence):\n",
        "      self.hidden = self.init_hidden(sentence.shape[1])\n",
        "\n",
        "      embedding1 = self.emb_drop(self.embedding(sentence[0]))\n",
        "      embedding1 = embedding1.permute(1, 0, 2)\n",
        "      embedding2 = self.emb_drop(self.embedding(sentence[1]))\n",
        "      embedding2 = embedding2.permute(1, 0, 2)\n",
        "\n",
        "      GRU_out_1, _ = self.gru(embedding1, self.hidden)\n",
        "      GRU_out_2, _ = self.gru(embedding2, self.hidden)\n",
        "\n",
        "      out = self.hidden_to_out(self.relu(torch.cat([GRU_out_1[-1], GRU_out_2[-1]], 1)))\n",
        "      return out\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros((4, batch_size, self.n_hidden), requires_grad=True).to(self.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo2GztW3gAAJ"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 16\n",
        "\n",
        "INPUT_DIM = len(vocab) + 1\n",
        "HIDDEN_DIM = 400 \n",
        "EMBEDDING_DIM = 300 \n",
        "BATCH_SIZE = 32\n",
        "OUTPUT_DIM = 3\n",
        "NUM_BI_LAYERS = 1\n",
        "NUM_LAYERS = 4\n",
        "\n",
        "## TODO: Figure out network dimensions\n",
        "model = GRU_classification(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, device, NUM_LAYERS, NUM_BI_LAYERS)\n",
        "print(\"Model initialised.\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGbJCIjogdku"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(train_loader, dev_loader, model, epochs, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAhN57qU2toN"
      },
      "source": [
        "## Pre-provided code for approach 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "V_l3AjbLm6zE"
      },
      "source": [
        "train_and_dev = train_df['edit1']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit1'], train_df['label'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "naive_model = MultinomialNB().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = naive_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = naive_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpKDFO8ym6zE"
      },
      "source": [
        "#### Baseline for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhcP7AD6m6zE"
      },
      "source": [
        "# Baseline for the task\n",
        "pred_baseline = torch.zeros(len(dev_y)) + 1  # 1 is most common class\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, torch.tensor(dev_y.values), True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}